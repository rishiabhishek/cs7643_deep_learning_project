{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2654de6b-e87b-4c4a-ba7d-9117d29b52b2",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43789a3-471a-4a09-8619-c8ce8ad74331",
   "metadata": {},
   "source": [
    "Houlsby et al., 2019 paper proved that adapters at lower layers have less impact than those at higher layers (see Figure 6 in the paper). The experiment was done by removing the adapter at various layers to see the fall of accuracy on validation data.\n",
    "\n",
    "This finding is in line with the popular fine-tuning strategy of focusing on upper layers. One intuition is that the lower layers extract lower-level features that are shared among tasks, while the higher layers build features that are unique to different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab185e3-8951-4f3f-838e-bf09892f72d0",
   "metadata": {},
   "source": [
    "# Problem statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd6807b-60ee-4274-8947-59944ab0e876",
   "metadata": {},
   "source": [
    "We will answer three questions:\n",
    "\n",
    "1) Can we get similar SoTA results with lesser parameters (than fixed adapter size throughout layers) by using smaller adapter size (i.e. number of units in the bottleneck) at lower layers and larger size at high layers?\n",
    "\n",
    "2) Can we get better SoTA results by using approximately the same number of parameters as the fixed size approach, but with bigger size at higher layers and smaller size at lower layers?\n",
    "\n",
    "3) Experiment with different adapter configurations (non-linearity, etc)\n",
    "\n",
    "We will be investigating only Houlsby task adapter architecture (Houlsby et al., 2019), NOT language adapter architecture (Pfeiffer et al., 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf36ef3-ed96-493d-89ad-21faa40b81bb",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19246a4-6f79-4c89-a85b-93d0cde769cc",
   "metadata": {},
   "source": [
    "We will use the following approaches to get the answers:\n",
    "\n",
    "1) We will be investigating with Sentiment Analysis task on SST-2 dataset\n",
    "\n",
    "2) We will use the pre-trained adapter https://adapterhub.ml/adapters/ukp/bert-base-uncased_sentiment_sst-2_houlsby/ as baseline for our performance measurement\n",
    "\n",
    "3) We will experiment with different adapter sizes and compare the performances against the baseline\n",
    "\n",
    "4) We will experiment with various adapter configurations and compare the performances against the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a37fc-1793-4d5d-883f-2ca0af66d4e4",
   "metadata": {},
   "source": [
    "# Pre-trained Houlsby adapter sentiment/sst-2@ukp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95892c7d-67f9-44d2-b2a5-ea5e74c1835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithHeads, AdapterConfig, pipeline, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "import transformers.adapters.composition as ac\n",
    "from datasets import load_dataset, load_metric\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf1a0f0-565c-4362-9ced-e55fc8a03c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT_LOCAL_PATH='./bert-base-uncased/'\n",
    "BERT_LOCAL_PATH='D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02d4fab0-d643-4abc-833c-9728d0b4a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdapterConfig(original_ln_before=False, original_ln_after=True, residual_before_ln=True, adapter_residual_before_ln=False, ln_before=False, ln_after=False, mh_adapter=True, output_adapter=True, non_linearity='swish', reduction_factor=16, inv_adapter=None, inv_adapter_reduction_factor=None, cross_adapter=False, leave_out=[])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to resolve adapter without the name of a model. Please specify model_name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16976/788848599.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdapterConfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"houlsby\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0madapter_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sentiment/sst-2@ukp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_active_adapters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs7643-proj\\lib\\site-packages\\transformers\\adapters\\model_mixin.py\u001b[0m in \u001b[0;36mload_adapter\u001b[1;34m(self, adapter_name_or_path, config, version, model_name, load_as, source, with_head, custom_weights_loaders, leave_out, id2label, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[0mleave_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleave_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[0mid2label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m         )\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs7643-proj\\lib\\site-packages\\transformers\\adapters\\model_mixin.py\u001b[0m in \u001b[0;36mload_adapter\u001b[1;34m(self, adapter_name_or_path, config, version, model_name, load_as, source, custom_weights_loaders, leave_out, id2label, **kwargs)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdapterLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         load_dir, load_name = loader.load(\n\u001b[1;32m--> 418\u001b[1;33m             \u001b[0madapter_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_as\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave_out\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mleave_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m         )\n\u001b[0;32m    420\u001b[0m         \u001b[1;31m# load additional custom weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs7643-proj\\lib\\site-packages\\transformers\\adapters\\loading.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, adapter_name_or_path, config, version, model_name, load_as, loading_info, leave_out, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0madapter_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequested_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         )\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs7643-proj\\lib\\site-packages\\transformers\\adapters\\utils.py\u001b[0m in \u001b[0;36mresolve_adapter_path\u001b[1;34m(adapter_name_or_path, model_name, adapter_config, version, source, **kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msource\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"ah\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m         return pull_from_hub(\n\u001b[1;32m--> 448\u001b[1;33m             \u001b[0madapter_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madapter_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madapter_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m         )\n\u001b[0;32m    450\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0msource\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"hf\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs7643-proj\\lib\\site-packages\\transformers\\adapters\\utils.py\u001b[0m in \u001b[0;36mpull_from_hub\u001b[1;34m(specifier, model_name, adapter_config, version, strict, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \"\"\"\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unable to resolve adapter without the name of a model. Please specify model_name.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m     \u001b[1;31m# resolve config if it's an identifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0madapter_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to resolve adapter without the name of a model. Please specify model_name."
     ]
    }
   ],
   "source": [
    "#model = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "config = AdapterConfig.load(\"houlsby\")\n",
    "print(config)\n",
    "adapter_name = model.load_adapter(\"sentiment/sst-2@ukp\", config=config)\n",
    "model.set_active_adapters(adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0dbd034-b13e-43d6-9888-634df5eeb002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset sst (C:\\Users\\sawro\\.cache\\huggingface\\datasets\\sst\\default\\1.0.0\\b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b5bfd4c-cf61-4599-bacb-f4786cfd48e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 2210\n",
      "Sample test data: {'sentence': 'Effective but too-tepid biopic', 'label': 0.5138900279998779, 'tokens': 'Effective|but|too-tepid|biopic', 'tree': '6|6|5|5|7|7|0'}\n",
      "Train size: 8544\n",
      "Sample train data: {'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'label': 0.6944400072097778, 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n",
      "valid size: 1101\n"
     ]
    }
   ],
   "source": [
    "print(\"Test size:\", dataset[\"test\"].num_rows)\n",
    "# Each complete sentence is annotated with a float label that indicates its level of positive sentiment from 0.0 to 1.0.\n",
    "# We can transform the above into a binary sentiment classification task by rounding each label to 0 or 1.\n",
    "print(\"Sample test data:\", dataset[\"test\"][0])\n",
    "\n",
    "print(\"Train size:\", dataset[\"train\"].num_rows)\n",
    "print(\"Sample train data:\", dataset[\"train\"][0])\n",
    "\n",
    "print(\"valid size:\", dataset[\"validation\"].num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29271bd5-1033-4921-98f1-c9df86eff063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_LOCAL_PATH, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42860032-9fe3-43ad-9f08-6155d01ca007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment/sst-2@ukp pre-trained adapter accuracy on SST-2 test data:  0.8705882352941177\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis = pipeline(task=\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# LABEL_0=negative; LABEL_1=positive (the adapter head_config.json has no label2id, that's why auto populated label_0 and _1)\n",
    "output_labels = {0: \"LABEL_0\", 1: \"LABEL_1\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"sentiment/sst-2@ukp pre-trained adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "989a2be9-70c9-4703-b72c-4a5b8b2d6495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.7958005666732788}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis = pipeline(task=\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "sentiment_analysis(dataset[\"test\"][0]['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5307405b-144a-4d70-8f73-eb9c792ad526",
   "metadata": {},
   "source": [
    "# Baseline: Pre-training Houlsby adapter with SST-2 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166dcb42-3c63-4ab8-ad55-7d3ad8f48118",
   "metadata": {},
   "source": [
    "This baseline Adapter in Houlsby architecture is trained on the binary SST task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c8c5ec-8812-4d2b-9dcf-1776a1f2030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2'.\n",
      "Loading cached processed dataset at C:\\Users\\sawro\\.cache\\huggingface\\datasets\\sst\\default\\1.0.0\\b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff\\cache-61fdd679db65da69.arrow\n",
      "Loading cached processed dataset at C:\\Users\\sawro\\.cache\\huggingface\\datasets\\sst\\default\\1.0.0\\b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff\\cache-304b5527df7d1727.arrow\n",
      "Loading cached processed dataset at C:\\Users\\sawro\\.cache\\huggingface\\datasets\\sst\\default\\1.0.0\\b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff\\cache-46f79ea108545315.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 30:42 < 30:43, 1.45 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.643679</td>\n",
       "      <td>0.585831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.466579</td>\n",
       "      <td>0.795640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.405690</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.404800</td>\n",
       "      <td>0.390768</td>\n",
       "      <td>0.828338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.399300</td>\n",
       "      <td>0.409879</td>\n",
       "      <td>0.823797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.390800</td>\n",
       "      <td>0.391416</td>\n",
       "      <td>0.833787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.381402</td>\n",
       "      <td>0.839237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.379500</td>\n",
       "      <td>0.392848</td>\n",
       "      <td>0.840145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.378500</td>\n",
       "      <td>0.382502</td>\n",
       "      <td>0.841054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.374400</td>\n",
       "      <td>0.385497</td>\n",
       "      <td>0.842870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-267\n",
      "Configuration saved in ./tests\\checkpoint-267\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-267\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-267\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-267\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-267\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-267\\sst-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-534\n",
      "Configuration saved in ./tests\\checkpoint-534\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-534\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-534\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-534\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-534\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-534\\sst-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-801\n",
      "Configuration saved in ./tests\\checkpoint-801\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-801\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-801\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-801\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-801\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-801\\sst-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-1068\n",
      "Configuration saved in ./tests\\checkpoint-1068\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1068\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-1068\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1068\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-1068\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1068\\sst-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-1335\n",
      "Configuration saved in ./tests\\checkpoint-1335\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1335\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-1335\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1335\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-1335\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1335\\sst-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-1602\n",
      "Configuration saved in ./tests\\checkpoint-1602\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1602\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-1602\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1602\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-1602\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1602\\sst-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-1869\n",
      "Configuration saved in ./tests\\checkpoint-1869\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1869\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-1869\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1869\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-1869\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-1869\\sst-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-2136\n",
      "Configuration saved in ./tests\\checkpoint-2136\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-2136\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-2136\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-2136\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-2136\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-2136\\sst-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-2403\n",
      "Configuration saved in ./tests\\checkpoint-2403\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-2403\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-2403\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-2403\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-2403\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-2403\\sst-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests\\checkpoint-2670\n",
      "Configuration saved in ./tests\\checkpoint-2670\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\checkpoint-2670\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\checkpoint-2670\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-2670\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\checkpoint-2670\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\checkpoint-2670\\sst-2\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests\\checkpoint-1869 (score: 0.38140231370925903).\n",
      "Loading module configuration from ./tests\\checkpoint-1869\\sst-2\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2'.\n",
      "Loading module weights from ./tests\\checkpoint-1869\\sst-2\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests\\checkpoint-1869\\sst-2\\head_config.json\n",
      "Overwriting existing head 'sst-2'\n",
      "Adding head 'sst-2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests\\checkpoint-1869\\sst-2\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests\\checkpoint-1869 (score: 0.38140231370925903).\n",
      "Saving model checkpoint to ./tests\n",
      "Configuration saved in ./tests\\sst-2\\adapter_config.json\n",
      "Module weights saved in ./tests\\sst-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\sst-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests\\sst-2\\head_config.json\n",
      "Module weights saved in ./tests\\sst-2\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "config2 = AdapterConfig.load(\"houlsby\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2\"])\n",
    "model2.set_active_adapters(\"sst-2\")\n",
    "#print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94249fb2-a8d2-464b-bc24-243bb84c9166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests/sst-2\\adapter_config.json\n",
      "Adding adapter 'sst-2'.\n",
      "Loading module weights from ./tests/sst-2\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests/sst-2\\head_config.json\n",
      "Overwriting existing head 'sst-2'\n",
      "Adding head 'sst-2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests/sst-2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline adapter accuracy on SST-2 test data:  0.8384615384615385\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained baseline adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests/sst-2/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests/sst-2\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"Baseline adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "968db168-9e98-42f6-8233-3dadf099d007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-1' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-1'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-1): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-1): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-1): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a135c64f6740b287986beca393a7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefebaaa5d5f401f9e23e939e1297e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4e80d23d0c4c9daa98f6ae29f44340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 33:11 < 33:12, 1.34 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.633846</td>\n",
       "      <td>0.573115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>0.414808</td>\n",
       "      <td>0.816530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.398112</td>\n",
       "      <td>0.827430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.397400</td>\n",
       "      <td>0.387888</td>\n",
       "      <td>0.834696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.408601</td>\n",
       "      <td>0.826521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.390064</td>\n",
       "      <td>0.839237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>0.380815</td>\n",
       "      <td>0.839237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.391442</td>\n",
       "      <td>0.838329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.373700</td>\n",
       "      <td>0.381776</td>\n",
       "      <td>0.847411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.366700</td>\n",
       "      <td>0.385589</td>\n",
       "      <td>0.841962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-267\n",
      "Configuration saved in ./tests-1\\checkpoint-267\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-267\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-267\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-267\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-267\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-267\\sst-2-1\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-534\n",
      "Configuration saved in ./tests-1\\checkpoint-534\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-534\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-534\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-534\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-534\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-534\\sst-2-1\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-801\n",
      "Configuration saved in ./tests-1\\checkpoint-801\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-801\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-801\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-801\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-801\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-801\\sst-2-1\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-1068\n",
      "Configuration saved in ./tests-1\\checkpoint-1068\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1068\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-1068\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1068\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-1068\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1068\\sst-2-1\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-1335\n",
      "Configuration saved in ./tests-1\\checkpoint-1335\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1335\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-1335\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1335\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-1335\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1335\\sst-2-1\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-1602\n",
      "Configuration saved in ./tests-1\\checkpoint-1602\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1602\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-1602\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1602\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-1602\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1602\\sst-2-1\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-1869\n",
      "Configuration saved in ./tests-1\\checkpoint-1869\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1869\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-1869\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1869\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-1869\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-1869\\sst-2-1\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-2136\n",
      "Configuration saved in ./tests-1\\checkpoint-2136\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-2136\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-2136\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-2136\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-2136\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-2136\\sst-2-1\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-2403\n",
      "Configuration saved in ./tests-1\\checkpoint-2403\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-2403\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-2403\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-2403\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-2403\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-2403\\sst-2-1\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-1\\checkpoint-2670\n",
      "Configuration saved in ./tests-1\\checkpoint-2670\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-2670\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-2670\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-2670\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\checkpoint-2670\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\checkpoint-2670\\sst-2-1\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-1\\checkpoint-1869 (score: 0.38081517815589905).\n",
      "Loading module configuration from ./tests-1\\checkpoint-1869\\sst-2-1\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-1'.\n",
      "Loading module weights from ./tests-1\\checkpoint-1869\\sst-2-1\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-1\\checkpoint-1869\\sst-2-1\\head_config.json\n",
      "Overwriting existing head 'sst-2-1'\n",
      "Adding head 'sst-2-1' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-1\\checkpoint-1869\\sst-2-1\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-1\\checkpoint-1869 (score: 0.38081517815589905).\n",
      "Saving model checkpoint to ./tests-1\n",
      "Configuration saved in ./tests-1\\sst-2-1\\adapter_config.json\n",
      "Module weights saved in ./tests-1\\sst-2-1\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-1\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\sst-2-1\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-1\\sst-2-1\\head_config.json\n",
      "Module weights saved in ./tests-1\\sst-2-1\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor=12)\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-1\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-1\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-1\"])\n",
    "model2.set_active_adapters(\"sst-2-1\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-1\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa99e53-5059-45ae-98ff-89e228b2f1ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18464/1406936872.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\" Evaluate the above pre-trained baseline adapter on SST-2 test data \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-uncased\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelWithHeads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-uncased\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained baseline adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-1/sst-2-1/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests-1/sst-2-1\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"62-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16cab956-5d0f-455b-8f19-9ca228fe2319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-2'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-2): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-2): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-2): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50840c915884b86b2a6c7e100beef31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45be288d32c9449dbe533818967e3799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce33d843f90848fc97f3376412c9dcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 28:22 < 28:23, 1.57 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685400</td>\n",
       "      <td>0.656857</td>\n",
       "      <td>0.541326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.615900</td>\n",
       "      <td>0.511956</td>\n",
       "      <td>0.788374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>0.409374</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.411800</td>\n",
       "      <td>0.393737</td>\n",
       "      <td>0.822888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.414343</td>\n",
       "      <td>0.813806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.394900</td>\n",
       "      <td>0.394450</td>\n",
       "      <td>0.823797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.388100</td>\n",
       "      <td>0.383094</td>\n",
       "      <td>0.831063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.383300</td>\n",
       "      <td>0.394562</td>\n",
       "      <td>0.830154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.383100</td>\n",
       "      <td>0.385316</td>\n",
       "      <td>0.842870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.378200</td>\n",
       "      <td>0.387420</td>\n",
       "      <td>0.839237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-267\n",
      "Configuration saved in ./tests-2\\checkpoint-267\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-267\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-267\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-267\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-267\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-267\\sst-2-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-534\n",
      "Configuration saved in ./tests-2\\checkpoint-534\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-534\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-534\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-534\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-534\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-534\\sst-2-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-801\n",
      "Configuration saved in ./tests-2\\checkpoint-801\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-801\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-801\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-801\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-801\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-801\\sst-2-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-1068\n",
      "Configuration saved in ./tests-2\\checkpoint-1068\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1068\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-1068\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1068\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-1068\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1068\\sst-2-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-1335\n",
      "Configuration saved in ./tests-2\\checkpoint-1335\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1335\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-1335\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1335\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-1335\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1335\\sst-2-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-1602\n",
      "Configuration saved in ./tests-2\\checkpoint-1602\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1602\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-1602\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1602\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-1602\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1602\\sst-2-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-1869\n",
      "Configuration saved in ./tests-2\\checkpoint-1869\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1869\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-1869\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1869\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-1869\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-1869\\sst-2-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-2136\n",
      "Configuration saved in ./tests-2\\checkpoint-2136\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-2136\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-2136\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-2136\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-2136\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-2136\\sst-2-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-2403\n",
      "Configuration saved in ./tests-2\\checkpoint-2403\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-2403\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-2403\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-2403\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-2403\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-2403\\sst-2-2\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-2\\checkpoint-2670\n",
      "Configuration saved in ./tests-2\\checkpoint-2670\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-2670\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-2670\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-2670\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\checkpoint-2670\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\checkpoint-2670\\sst-2-2\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-2\\checkpoint-1869 (score: 0.38309356570243835).\n",
      "Loading module configuration from ./tests-2\\checkpoint-1869\\sst-2-2\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-2'.\n",
      "Loading module weights from ./tests-2\\checkpoint-1869\\sst-2-2\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-2\\checkpoint-1869\\sst-2-2\\head_config.json\n",
      "Overwriting existing head 'sst-2-2'\n",
      "Adding head 'sst-2-2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-2\\checkpoint-1869\\sst-2-2\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-2\\checkpoint-1869 (score: 0.38309356570243835).\n",
      "Saving model checkpoint to ./tests-2\n",
      "Configuration saved in ./tests-2\\sst-2-2\\adapter_config.json\n",
      "Module weights saved in ./tests-2\\sst-2-2\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-2\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\sst-2-2\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-2\\sst-2-2\\head_config.json\n",
      "Module weights saved in ./tests-2\\sst-2-2\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "# Layer 0 - 3 = 24 adapter size; the rest = 48\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'0':32, '1':32,'2':32,'3':32,'default':16})\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-2\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-2\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-2\"])\n",
    "model2.set_active_adapters(\"sst-2-2\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-2\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5fffea4-f626-4713-88bb-0d4cff687859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-2' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-2/sst-2-2\\adapter_config.json\n",
      "Adding adapter 'sst-2-2'.\n",
      "Loading module weights from ./tests-2/sst-2-2\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-2/sst-2-2\\head_config.json\n",
      "Overwriting existing head 'sst-2-2'\n",
      "Adding head 'sst-2-2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-2/sst-2-2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8357466063348417\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-2',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-2/sst-2-2/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests-2/sst-2-2\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74b047e9-214d-49cb-9437-a71f4bcc2056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-3' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-3'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-3): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-3): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-3): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e904d378bec8407c83533b96ef952729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38f11fd35434babbca2f3748b1ee051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f57b0fd8184f768d004a9634bb4a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 28:20 < 28:22, 1.57 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.650549</td>\n",
       "      <td>0.550409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>0.499978</td>\n",
       "      <td>0.790191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.407693</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.396169</td>\n",
       "      <td>0.825613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.405300</td>\n",
       "      <td>0.413016</td>\n",
       "      <td>0.819255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.396300</td>\n",
       "      <td>0.395992</td>\n",
       "      <td>0.825613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.388800</td>\n",
       "      <td>0.385401</td>\n",
       "      <td>0.840145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.385200</td>\n",
       "      <td>0.395321</td>\n",
       "      <td>0.835604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.387065</td>\n",
       "      <td>0.840145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.379900</td>\n",
       "      <td>0.389256</td>\n",
       "      <td>0.840145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-267\n",
      "Configuration saved in ./tests-3\\checkpoint-267\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-267\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-267\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-267\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-267\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-267\\sst-2-3\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-534\n",
      "Configuration saved in ./tests-3\\checkpoint-534\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-534\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-534\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-534\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-534\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-534\\sst-2-3\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-801\n",
      "Configuration saved in ./tests-3\\checkpoint-801\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-801\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-801\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-801\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-801\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-801\\sst-2-3\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-1068\n",
      "Configuration saved in ./tests-3\\checkpoint-1068\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1068\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-1068\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1068\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-1068\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1068\\sst-2-3\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-1335\n",
      "Configuration saved in ./tests-3\\checkpoint-1335\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1335\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-1335\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1335\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-1335\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1335\\sst-2-3\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-1602\n",
      "Configuration saved in ./tests-3\\checkpoint-1602\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1602\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-1602\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1602\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-1602\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1602\\sst-2-3\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-1869\n",
      "Configuration saved in ./tests-3\\checkpoint-1869\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1869\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-1869\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1869\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-1869\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-1869\\sst-2-3\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-2136\n",
      "Configuration saved in ./tests-3\\checkpoint-2136\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-2136\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-2136\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-2136\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-2136\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-2136\\sst-2-3\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-2403\n",
      "Configuration saved in ./tests-3\\checkpoint-2403\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-2403\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-2403\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-2403\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-2403\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-2403\\sst-2-3\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-3\\checkpoint-2670\n",
      "Configuration saved in ./tests-3\\checkpoint-2670\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-2670\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-2670\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-2670\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\checkpoint-2670\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\checkpoint-2670\\sst-2-3\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-3\\checkpoint-1869 (score: 0.3854014277458191).\n",
      "Loading module configuration from ./tests-3\\checkpoint-1869\\sst-2-3\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-3'.\n",
      "Loading module weights from ./tests-3\\checkpoint-1869\\sst-2-3\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-3\\checkpoint-1869\\sst-2-3\\head_config.json\n",
      "Overwriting existing head 'sst-2-3'\n",
      "Adding head 'sst-2-3' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-3\\checkpoint-1869\\sst-2-3\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-3\\checkpoint-1869 (score: 0.3854014277458191).\n",
      "Saving model checkpoint to ./tests-3\n",
      "Configuration saved in ./tests-3\\sst-2-3\\adapter_config.json\n",
      "Module weights saved in ./tests-3\\sst-2-3\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-3\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\sst-2-3\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-3\\sst-2-3\\head_config.json\n",
      "Module weights saved in ./tests-3\\sst-2-3\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "# Layer 0 - 5 = 24 adapter size; the rest = 48; total 12 layers\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'0':32, '1':32,'2':32,'3':32,'4':32,'5':32,'default':16})\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-3\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-3\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-3\"])\n",
    "model2.set_active_adapters(\"sst-2-3\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-3\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e1a902-d147-40bc-9aa2-305ed85eaec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Overwriting existing head 'sst-2-3'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8361990950226245\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-3',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-3/sst-2-3/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests-3/sst-2-3\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1dfa721-d29c-4f19-8666-c9a22e9690c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-4): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-4): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-4): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b85b42e00441ab992c51b7d2555bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130b82528af3498dadff19e9ba332e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2290334252ac46588df05e1b5ac7732c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 32:44 < 32:46, 1.36 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>0.641744</td>\n",
       "      <td>0.569482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.489704</td>\n",
       "      <td>0.790191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.406582</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.395365</td>\n",
       "      <td>0.821980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>0.412662</td>\n",
       "      <td>0.814714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.397122</td>\n",
       "      <td>0.833787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.389700</td>\n",
       "      <td>0.388018</td>\n",
       "      <td>0.840145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.397658</td>\n",
       "      <td>0.831063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.388746</td>\n",
       "      <td>0.841962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>0.391042</td>\n",
       "      <td>0.842870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-267\n",
      "Configuration saved in ./tests-4\\checkpoint-267\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-267\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-267\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-267\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-267\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-267\\sst-2-4\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-534\n",
      "Configuration saved in ./tests-4\\checkpoint-534\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-534\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-534\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-534\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-534\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-534\\sst-2-4\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-801\n",
      "Configuration saved in ./tests-4\\checkpoint-801\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-801\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-801\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-801\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-801\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-801\\sst-2-4\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-1068\n",
      "Configuration saved in ./tests-4\\checkpoint-1068\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1068\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-1068\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1068\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-1068\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1068\\sst-2-4\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-1335\n",
      "Configuration saved in ./tests-4\\checkpoint-1335\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1335\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-1335\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1335\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-1335\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1335\\sst-2-4\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-1602\n",
      "Configuration saved in ./tests-4\\checkpoint-1602\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1602\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-1602\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1602\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-1602\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1602\\sst-2-4\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-1869\n",
      "Configuration saved in ./tests-4\\checkpoint-1869\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1869\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-1869\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1869\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-1869\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-1869\\sst-2-4\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-2136\n",
      "Configuration saved in ./tests-4\\checkpoint-2136\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-2136\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-2136\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-2136\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-2136\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-2136\\sst-2-4\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-2403\n",
      "Configuration saved in ./tests-4\\checkpoint-2403\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-2403\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-2403\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-2403\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-2403\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-2403\\sst-2-4\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-4\\checkpoint-2670\n",
      "Configuration saved in ./tests-4\\checkpoint-2670\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-2670\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-2670\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-2670\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\checkpoint-2670\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\checkpoint-2670\\sst-2-4\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-4\\checkpoint-1869 (score: 0.38801833987236023).\n",
      "Loading module configuration from ./tests-4\\checkpoint-1869\\sst-2-4\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-4'.\n",
      "Loading module weights from ./tests-4\\checkpoint-1869\\sst-2-4\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-4\\checkpoint-1869\\sst-2-4\\head_config.json\n",
      "Overwriting existing head 'sst-2-4'\n",
      "Adding head 'sst-2-4' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-4\\checkpoint-1869\\sst-2-4\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-4\\checkpoint-1869 (score: 0.38801833987236023).\n",
      "Saving model checkpoint to ./tests-4\n",
      "Configuration saved in ./tests-4\\sst-2-4\\adapter_config.json\n",
      "Module weights saved in ./tests-4\\sst-2-4\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-4\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\sst-2-4\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-4\\sst-2-4\\head_config.json\n",
      "Module weights saved in ./tests-4\\sst-2-4\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "# Layer 0 - 7 = 24 adapter size; the rest = 48; total 12 layers\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'0':32, '1':32,'2':32,'3':32,'4':32,'5':32,'6':32,'7':32,'default':16})\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-4\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-4\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-4\"])\n",
    "model2.set_active_adapters(\"sst-2-4\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-4\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdea6e14-073b-4d57-b170-4d91310f3c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-4' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-4/sst-2-4\\adapter_config.json\n",
      "Adding adapter 'sst-2-4'.\n",
      "Loading module weights from ./tests-4/sst-2-4\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-4/sst-2-4\\head_config.json\n",
      "Overwriting existing head 'sst-2-4'\n",
      "Adding head 'sst-2-4' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-4/sst-2-4\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8366515837104073\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-4',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-4/sst-2-4/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests-4/sst-2-4\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab09364b-ed1e-426e-a6a5-5ed49e713058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-5' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-5'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=24, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=24, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-5): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-5): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-5): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aeb262675244f5796e8dc3cf61b20e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a2287ec51e4f969da47fc72325107d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a287ef02df4c748b66981daa468c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 29:18 < 29:20, 1.52 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>0.649141</td>\n",
       "      <td>0.532243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.511369</td>\n",
       "      <td>0.782016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.447700</td>\n",
       "      <td>0.413461</td>\n",
       "      <td>0.808356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.412100</td>\n",
       "      <td>0.397808</td>\n",
       "      <td>0.816530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.408500</td>\n",
       "      <td>0.418374</td>\n",
       "      <td>0.821980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.400700</td>\n",
       "      <td>0.399722</td>\n",
       "      <td>0.824705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.393400</td>\n",
       "      <td>0.389604</td>\n",
       "      <td>0.832879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.388800</td>\n",
       "      <td>0.401028</td>\n",
       "      <td>0.829246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.388500</td>\n",
       "      <td>0.392585</td>\n",
       "      <td>0.832879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.393548</td>\n",
       "      <td>0.834696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-267\n",
      "Configuration saved in ./tests-5\\checkpoint-267\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-267\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-267\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-267\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-267\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-267\\sst-2-5\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-534\n",
      "Configuration saved in ./tests-5\\checkpoint-534\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-534\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-534\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-534\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-534\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-534\\sst-2-5\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-801\n",
      "Configuration saved in ./tests-5\\checkpoint-801\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-801\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-801\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-801\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-801\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-801\\sst-2-5\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-1068\n",
      "Configuration saved in ./tests-5\\checkpoint-1068\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1068\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-1068\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1068\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-1068\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1068\\sst-2-5\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-1335\n",
      "Configuration saved in ./tests-5\\checkpoint-1335\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1335\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-1335\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1335\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-1335\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1335\\sst-2-5\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-1602\n",
      "Configuration saved in ./tests-5\\checkpoint-1602\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1602\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-1602\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1602\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-1602\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1602\\sst-2-5\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-1869\n",
      "Configuration saved in ./tests-5\\checkpoint-1869\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1869\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-1869\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1869\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-1869\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-1869\\sst-2-5\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-2136\n",
      "Configuration saved in ./tests-5\\checkpoint-2136\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-2136\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-2136\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-2136\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-2136\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-2136\\sst-2-5\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-2403\n",
      "Configuration saved in ./tests-5\\checkpoint-2403\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-2403\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-2403\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-2403\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-2403\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-2403\\sst-2-5\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-5\\checkpoint-2670\n",
      "Configuration saved in ./tests-5\\checkpoint-2670\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-2670\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-2670\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-2670\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\checkpoint-2670\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\checkpoint-2670\\sst-2-5\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-5\\checkpoint-1869 (score: 0.3896041810512543).\n",
      "Loading module configuration from ./tests-5\\checkpoint-1869\\sst-2-5\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-5'.\n",
      "Loading module weights from ./tests-5\\checkpoint-1869\\sst-2-5\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-5\\checkpoint-1869\\sst-2-5\\head_config.json\n",
      "Overwriting existing head 'sst-2-5'\n",
      "Adding head 'sst-2-5' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-5\\checkpoint-1869\\sst-2-5\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-5\\checkpoint-1869 (score: 0.3896041810512543).\n",
      "Saving model checkpoint to ./tests-5\n",
      "Configuration saved in ./tests-5\\sst-2-5\\adapter_config.json\n",
      "Module weights saved in ./tests-5\\sst-2-5\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-5\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\sst-2-5\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-5\\sst-2-5\\head_config.json\n",
      "Module weights saved in ./tests-5\\sst-2-5\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "# Layer 10 & 11 (last 2 layers) = 48 adapter size; the rest = 24; total 12 layers\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'10':16,'11':16, 'default': 32})\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-5\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-5\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-5\"])\n",
    "model2.set_active_adapters(\"sst-2-5\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-5\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de021830-194b-48dc-9104-5920e259a331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-5' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-5/sst-2-5\\adapter_config.json\n",
      "Adding adapter 'sst-2-5'.\n",
      "Loading module weights from ./tests-5/sst-2-5\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-5/sst-2-5\\head_config.json\n",
      "Overwriting existing head 'sst-2-5'\n",
      "Adding head 'sst-2-5' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-5/sst-2-5\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8343891402714932\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-5',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-5/sst-2-5/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests-5/sst-2-5\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "597e9e23-1db4-488a-85fc-97117d422b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-6' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-6'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-6): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-6): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-6): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d89bc2947c4a6f93e6a71982db87f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3883837f3536422dbaaca4ff5b64df73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753bcf8983d4459daae4ec61e0b8cedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 28:27 < 28:28, 1.56 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>0.645690</td>\n",
       "      <td>0.548592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.614300</td>\n",
       "      <td>0.509996</td>\n",
       "      <td>0.776567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.451800</td>\n",
       "      <td>0.412865</td>\n",
       "      <td>0.815622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.413700</td>\n",
       "      <td>0.398721</td>\n",
       "      <td>0.817439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.418260</td>\n",
       "      <td>0.815622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.398249</td>\n",
       "      <td>0.830154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.392400</td>\n",
       "      <td>0.388743</td>\n",
       "      <td>0.831971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.390700</td>\n",
       "      <td>0.399008</td>\n",
       "      <td>0.839237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.389700</td>\n",
       "      <td>0.390280</td>\n",
       "      <td>0.840145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.386200</td>\n",
       "      <td>0.393056</td>\n",
       "      <td>0.842870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-267\n",
      "Configuration saved in ./tests-6\\checkpoint-267\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-267\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-267\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-267\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-267\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-267\\sst-2-6\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-534\n",
      "Configuration saved in ./tests-6\\checkpoint-534\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-534\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-534\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-534\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-534\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-534\\sst-2-6\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-801\n",
      "Configuration saved in ./tests-6\\checkpoint-801\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-801\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-801\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-801\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-801\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-801\\sst-2-6\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-1068\n",
      "Configuration saved in ./tests-6\\checkpoint-1068\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1068\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1068\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1068\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1068\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1068\\sst-2-6\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-1335\n",
      "Configuration saved in ./tests-6\\checkpoint-1335\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1335\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1335\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1335\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1335\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1335\\sst-2-6\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-1602\n",
      "Configuration saved in ./tests-6\\checkpoint-1602\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1602\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1602\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1602\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1602\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1602\\sst-2-6\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-1869\n",
      "Configuration saved in ./tests-6\\checkpoint-1869\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1869\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1869\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1869\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1869\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1869\\sst-2-6\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-2136\n",
      "Configuration saved in ./tests-6\\checkpoint-2136\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2136\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2136\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2136\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2136\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2136\\sst-2-6\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-2403\n",
      "Configuration saved in ./tests-6\\checkpoint-2403\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2403\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2403\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2403\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2403\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2403\\sst-2-6\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-2670\n",
      "Configuration saved in ./tests-6\\checkpoint-2670\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2670\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2670\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2670\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2670\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2670\\sst-2-6\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-6\\checkpoint-1869 (score: 0.3887428939342499).\n",
      "Loading module configuration from ./tests-6\\checkpoint-1869\\sst-2-6\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-6'.\n",
      "Loading module weights from ./tests-6\\checkpoint-1869\\sst-2-6\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-6\\checkpoint-1869\\sst-2-6\\head_config.json\n",
      "Overwriting existing head 'sst-2-6'\n",
      "Adding head 'sst-2-6' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-6\\checkpoint-1869\\sst-2-6\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-6\\checkpoint-1869 (score: 0.3887428939342499).\n",
      "Saving model checkpoint to ./tests-6\n",
      "Configuration saved in ./tests-6\\sst-2-6\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\sst-2-6\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\sst-2-6\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\sst-2-6\\head_config.json\n",
      "Module weights saved in ./tests-6\\sst-2-6\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'0':48, '1':48,'2':48,'3':48,'4':48,'5':48,'6':48,'7':48,'default':16})\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-6\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-6\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-6\"])\n",
    "model2.set_active_adapters(\"sst-2-6\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-6\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74747838-b2f3-4d10-867d-473b6b7036bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-6' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-6/sst-2-6\\adapter_config.json\n",
      "Adding adapter 'sst-2-6'.\n",
      "Loading module weights from ./tests-6/sst-2-6\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-6/sst-2-6\\head_config.json\n",
      "Overwriting existing head 'sst-2-6'\n",
      "Adding head 'sst-2-6' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-6/sst-2-6\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8357466063348417\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-6',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-6/sst-2-6/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests-6/sst-2-6\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2753c1-e677-43ee-ad91-29bc9969382c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-7' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-7'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-7): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-7): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-7): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d608703a403343c68a1b06856c4cc5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce81b3cc0a84f83bd7e8c15ea0dafa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad867ec3495493bb5ec99bd3621905c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 28:29 < 28:30, 1.56 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>0.634971</td>\n",
       "      <td>0.590372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.563200</td>\n",
       "      <td>0.441669</td>\n",
       "      <td>0.809264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.419800</td>\n",
       "      <td>0.398580</td>\n",
       "      <td>0.827430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>0.389816</td>\n",
       "      <td>0.830154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.401400</td>\n",
       "      <td>0.408022</td>\n",
       "      <td>0.827430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.390400</td>\n",
       "      <td>0.390612</td>\n",
       "      <td>0.836512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.384100</td>\n",
       "      <td>0.381206</td>\n",
       "      <td>0.838329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.391488</td>\n",
       "      <td>0.838329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.379500</td>\n",
       "      <td>0.383406</td>\n",
       "      <td>0.842870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.384386</td>\n",
       "      <td>0.842870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-267\n",
      "Configuration saved in ./tests-6\\checkpoint-267\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-267\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-267\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-267\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-267\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-267\\sst-2-7\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-534\n",
      "Configuration saved in ./tests-6\\checkpoint-534\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-534\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-534\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-534\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-534\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-534\\sst-2-7\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-801\n",
      "Configuration saved in ./tests-6\\checkpoint-801\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-801\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-801\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-801\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-801\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-801\\sst-2-7\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-1068\n",
      "Configuration saved in ./tests-6\\checkpoint-1068\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1068\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1068\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1068\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1068\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1068\\sst-2-7\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-1335\n",
      "Configuration saved in ./tests-6\\checkpoint-1335\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1335\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1335\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1335\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1335\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1335\\sst-2-7\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-1602\n",
      "Configuration saved in ./tests-6\\checkpoint-1602\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1602\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1602\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1602\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1602\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1602\\sst-2-7\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-1869\n",
      "Configuration saved in ./tests-6\\checkpoint-1869\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1869\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1869\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1869\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-1869\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-1869\\sst-2-7\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-2136\n",
      "Configuration saved in ./tests-6\\checkpoint-2136\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2136\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2136\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2136\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2136\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2136\\sst-2-7\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-2403\n",
      "Configuration saved in ./tests-6\\checkpoint-2403\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2403\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2403\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2403\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2403\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2403\\sst-2-7\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-6\\checkpoint-2670\n",
      "Configuration saved in ./tests-6\\checkpoint-2670\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2670\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2670\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2670\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\checkpoint-2670\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\checkpoint-2670\\sst-2-7\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-6\\checkpoint-1869 (score: 0.38120582699775696).\n",
      "Loading module configuration from ./tests-6\\checkpoint-1869\\sst-2-7\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-7'.\n",
      "Loading module weights from ./tests-6\\checkpoint-1869\\sst-2-7\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-6\\checkpoint-1869\\sst-2-7\\head_config.json\n",
      "Overwriting existing head 'sst-2-7'\n",
      "Adding head 'sst-2-7' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-6\\checkpoint-1869\\sst-2-7\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-6\\checkpoint-1869 (score: 0.38120582699775696).\n",
      "Saving model checkpoint to ./tests-6\n",
      "Configuration saved in ./tests-6\\sst-2-7\\adapter_config.json\n",
      "Module weights saved in ./tests-6\\sst-2-7\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-6\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\sst-2-7\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-6\\sst-2-7\\head_config.json\n",
      "Module weights saved in ./tests-6\\sst-2-7\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'10':96, '11':96,'default':16})\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-7\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-7\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-7\"])\n",
    "model2.set_active_adapters(\"sst-2-7\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-7\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28e4edc4-3edc-4c1e-b424-88703c7f5e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-7' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-7/sst-2-7\\adapter_config.json\n",
      "Adding adapter 'sst-2-7'.\n",
      "Loading module weights from ./tests-7/sst-2-7\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-7/sst-2-7\\head_config.json\n",
      "Overwriting existing head 'sst-2-7'\n",
      "Adding head 'sst-2-7' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-7/sst-2-7\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8371040723981901\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-7',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-7/sst-2-7/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests-7/sst-2-7\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf99d24-7eaf-4079-af81-74b31701bbb6",
   "metadata": {},
   "source": [
    "## Interesting Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffec9b88-9611-4308-a90d-dc1f56e27236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-8' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-8'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-8): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-8): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-8): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17db9fdf88d541dfb863686fb982ea9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359db8d655c8444b9c0eff881232bdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d51e661a52445ff88c3a828130afb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 28:11 < 28:12, 1.58 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685100</td>\n",
       "      <td>0.661800</td>\n",
       "      <td>0.518619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.654100</td>\n",
       "      <td>0.594929</td>\n",
       "      <td>0.722071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.569300</td>\n",
       "      <td>0.487377</td>\n",
       "      <td>0.789282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.458300</td>\n",
       "      <td>0.419962</td>\n",
       "      <td>0.802906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.428200</td>\n",
       "      <td>0.430441</td>\n",
       "      <td>0.804723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.416300</td>\n",
       "      <td>0.411480</td>\n",
       "      <td>0.814714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.401943</td>\n",
       "      <td>0.817439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.412195</td>\n",
       "      <td>0.819255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.404800</td>\n",
       "      <td>0.403658</td>\n",
       "      <td>0.823797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.404100</td>\n",
       "      <td>0.404804</td>\n",
       "      <td>0.825613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-267\n",
      "Configuration saved in ./tests-8\\checkpoint-267\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-267\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-267\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-267\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-267\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-267\\sst-2-8\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-534\n",
      "Configuration saved in ./tests-8\\checkpoint-534\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-534\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-534\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-534\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-534\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-534\\sst-2-8\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-801\n",
      "Configuration saved in ./tests-8\\checkpoint-801\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-801\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-801\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-801\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-801\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-801\\sst-2-8\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-1068\n",
      "Configuration saved in ./tests-8\\checkpoint-1068\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1068\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-1068\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1068\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-1068\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1068\\sst-2-8\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-1335\n",
      "Configuration saved in ./tests-8\\checkpoint-1335\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1335\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-1335\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1335\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-1335\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1335\\sst-2-8\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-1602\n",
      "Configuration saved in ./tests-8\\checkpoint-1602\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1602\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-1602\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1602\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-1602\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1602\\sst-2-8\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-1869\n",
      "Configuration saved in ./tests-8\\checkpoint-1869\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1869\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-1869\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1869\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-1869\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-1869\\sst-2-8\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-2136\n",
      "Configuration saved in ./tests-8\\checkpoint-2136\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-2136\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-2136\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-2136\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-2136\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-2136\\sst-2-8\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-2403\n",
      "Configuration saved in ./tests-8\\checkpoint-2403\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-2403\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-2403\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-2403\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-2403\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-2403\\sst-2-8\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-8\\checkpoint-2670\n",
      "Configuration saved in ./tests-8\\checkpoint-2670\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-2670\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-2670\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-2670\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\checkpoint-2670\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\checkpoint-2670\\sst-2-8\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-8\\checkpoint-1869 (score: 0.401943176984787).\n",
      "Loading module configuration from ./tests-8\\checkpoint-1869\\sst-2-8\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-8'.\n",
      "Loading module weights from ./tests-8\\checkpoint-1869\\sst-2-8\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-8\\checkpoint-1869\\sst-2-8\\head_config.json\n",
      "Overwriting existing head 'sst-2-8'\n",
      "Adding head 'sst-2-8' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-8\\checkpoint-1869\\sst-2-8\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-8\\checkpoint-1869 (score: 0.401943176984787).\n",
      "Saving model checkpoint to ./tests-8\n",
      "Configuration saved in ./tests-8\\sst-2-8\\adapter_config.json\n",
      "Module weights saved in ./tests-8\\sst-2-8\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-8\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\sst-2-8\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-8\\sst-2-8\\head_config.json\n",
      "Module weights saved in ./tests-8\\sst-2-8\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'11':48,'default':96})\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-8\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-8\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-8\"])\n",
    "model2.set_active_adapters(\"sst-2-8\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-8\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e183913-39fd-4310-a9d0-c69f5633cb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-8' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-8/sst-2-8\\adapter_config.json\n",
      "Adding adapter 'sst-2-8'.\n",
      "Loading module weights from ./tests-8/sst-2-8\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-8/sst-2-8\\head_config.json\n",
      "Overwriting existing head 'sst-2-8'\n",
      "Adding head 'sst-2-8' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-8/sst-2-8\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8294117647058824\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-8',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-8/sst-2-8/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests-8/sst-2-8\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09aa2e47-44a2-440a-ba45-5236ee897274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-9' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-9'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=8, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=8, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-9): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-9): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-9): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc4e95d16dd49fea2bac94fe13d2b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0442e571256b47c4b145bc7c40601501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef7d068d28c42de83c4d8d0f45fa156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2670' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2670/5340 28:13 < 28:14, 1.58 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.684100</td>\n",
       "      <td>0.656445</td>\n",
       "      <td>0.542234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>0.742053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.492446</td>\n",
       "      <td>0.784741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.419760</td>\n",
       "      <td>0.803815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.430902</td>\n",
       "      <td>0.806540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.416300</td>\n",
       "      <td>0.411443</td>\n",
       "      <td>0.815622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.409200</td>\n",
       "      <td>0.402027</td>\n",
       "      <td>0.823797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.411619</td>\n",
       "      <td>0.822888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.402755</td>\n",
       "      <td>0.830154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.403245</td>\n",
       "      <td>0.832879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-267\n",
      "Configuration saved in ./tests-9\\checkpoint-267\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-267\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-267\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-267\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-267\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-267\\sst-2-9\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-534\n",
      "Configuration saved in ./tests-9\\checkpoint-534\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-534\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-534\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-534\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-534\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-534\\sst-2-9\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-801\n",
      "Configuration saved in ./tests-9\\checkpoint-801\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-801\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-801\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-801\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-801\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-801\\sst-2-9\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-1068\n",
      "Configuration saved in ./tests-9\\checkpoint-1068\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1068\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-1068\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1068\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-1068\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1068\\sst-2-9\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-1335\n",
      "Configuration saved in ./tests-9\\checkpoint-1335\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1335\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-1335\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1335\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-1335\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1335\\sst-2-9\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-1602\n",
      "Configuration saved in ./tests-9\\checkpoint-1602\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1602\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-1602\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1602\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-1602\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1602\\sst-2-9\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-1869\n",
      "Configuration saved in ./tests-9\\checkpoint-1869\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1869\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-1869\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1869\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-1869\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-1869\\sst-2-9\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-2136\n",
      "Configuration saved in ./tests-9\\checkpoint-2136\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-2136\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-2136\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-2136\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-2136\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-2136\\sst-2-9\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-2403\n",
      "Configuration saved in ./tests-9\\checkpoint-2403\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-2403\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-2403\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-2403\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-2403\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-2403\\sst-2-9\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: sentence, tokens, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-9\\checkpoint-2670\n",
      "Configuration saved in ./tests-9\\checkpoint-2670\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-2670\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-2670\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-2670\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\checkpoint-2670\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\checkpoint-2670\\sst-2-9\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-9\\checkpoint-1869 (score: 0.4020270109176636).\n",
      "Loading module configuration from ./tests-9\\checkpoint-1869\\sst-2-9\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-9'.\n",
      "Loading module weights from ./tests-9\\checkpoint-1869\\sst-2-9\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-9\\checkpoint-1869\\sst-2-9\\head_config.json\n",
      "Overwriting existing head 'sst-2-9'\n",
      "Adding head 'sst-2-9' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-9\\checkpoint-1869\\sst-2-9\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-9\\checkpoint-1869 (score: 0.4020270109176636).\n",
      "Saving model checkpoint to ./tests-9\n",
      "Configuration saved in ./tests-9\\sst-2-9\\adapter_config.json\n",
      "Module weights saved in ./tests-9\\sst-2-9\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-9\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\sst-2-9\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-9\\sst-2-9\\head_config.json\n",
      "Module weights saved in ./tests-9\\sst-2-9\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'10':48,'11':48,'default':96})\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-9\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-9\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-9\"])\n",
    "model2.set_active_adapters(\"sst-2-9\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-9\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78c6b402-4e2b-4ba6-a2d1-7c91742a61c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\sawro/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-9' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-9/sst-2-9\\adapter_config.json\n",
      "Adding adapter 'sst-2-9'.\n",
      "Loading module weights from ./tests-9/sst-2-9\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-9/sst-2-9\\head_config.json\n",
      "Overwriting existing head 'sst-2-9'\n",
      "Adding head 'sst-2-9' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-9/sst-2-9\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8262443438914027\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-9',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-9/sst-2-9/adapter_config.json\")\n",
    "adapter_name = model2.load_adapter(\"./tests-9/sst-2-9\", config=config)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdeb22fb-5b9b-47ba-a0de-3e88e52f72d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-10): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-10): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-10): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521099a6236742cfa40a83d8a7031d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d6559feaee4a699a084eade203724a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fabf79607c4a92a2e8c1b14a854930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301, 1005, 1055, 2047, 1036, 1036, 16608, 1005, 1005, 1998, 2008, 2002, 1005, 1055, 2183, 2000, 2191, 1037, 17624, 2130, 3618, 2084, 7779, 29058, 8625, 13327, 1010, 3744, 1011, 18856, 19513, 3158, 5477, 4168, 2030, 7112, 16562, 2140, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5340' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5340/5340 30:28, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.681100</td>\n",
       "      <td>0.652771</td>\n",
       "      <td>0.559491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>0.614935</td>\n",
       "      <td>0.696639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.588942</td>\n",
       "      <td>0.717530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.638700</td>\n",
       "      <td>0.559432</td>\n",
       "      <td>0.752044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.629900</td>\n",
       "      <td>0.547342</td>\n",
       "      <td>0.736603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.618400</td>\n",
       "      <td>0.527232</td>\n",
       "      <td>0.747502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>0.508385</td>\n",
       "      <td>0.758401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.602000</td>\n",
       "      <td>0.506527</td>\n",
       "      <td>0.751135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.596800</td>\n",
       "      <td>0.495741</td>\n",
       "      <td>0.762943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.588600</td>\n",
       "      <td>0.488556</td>\n",
       "      <td>0.763851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.585700</td>\n",
       "      <td>0.482575</td>\n",
       "      <td>0.769301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.582100</td>\n",
       "      <td>0.476238</td>\n",
       "      <td>0.772934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.579200</td>\n",
       "      <td>0.473223</td>\n",
       "      <td>0.772025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.572900</td>\n",
       "      <td>0.473874</td>\n",
       "      <td>0.773842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.576200</td>\n",
       "      <td>0.470824</td>\n",
       "      <td>0.776567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.473976</td>\n",
       "      <td>0.771117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.572900</td>\n",
       "      <td>0.471286</td>\n",
       "      <td>0.772934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.569100</td>\n",
       "      <td>0.470224</td>\n",
       "      <td>0.771117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.567200</td>\n",
       "      <td>0.470227</td>\n",
       "      <td>0.772025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.573900</td>\n",
       "      <td>0.470209</td>\n",
       "      <td>0.772934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-267\n",
      "Configuration saved in ./tests-10\\checkpoint-267\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-267\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-267\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-267\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-267\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-267\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-534\n",
      "Configuration saved in ./tests-10\\checkpoint-534\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-534\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-534\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-534\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-534\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-534\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-801\n",
      "Configuration saved in ./tests-10\\checkpoint-801\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-801\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-801\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-801\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-801\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-801\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-1068\n",
      "Configuration saved in ./tests-10\\checkpoint-1068\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1068\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1068\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1068\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1068\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1068\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-1335\n",
      "Configuration saved in ./tests-10\\checkpoint-1335\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1335\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1335\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1335\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1335\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1335\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-1602\n",
      "Configuration saved in ./tests-10\\checkpoint-1602\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1602\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1602\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1602\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1602\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1602\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-1869\n",
      "Configuration saved in ./tests-10\\checkpoint-1869\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1869\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1869\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1869\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1869\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1869\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-2136\n",
      "Configuration saved in ./tests-10\\checkpoint-2136\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2136\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2136\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2136\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2136\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2136\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-2403\n",
      "Configuration saved in ./tests-10\\checkpoint-2403\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2403\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2403\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2403\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2403\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2403\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-2670\n",
      "Configuration saved in ./tests-10\\checkpoint-2670\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2670\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2670\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2670\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2670\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2670\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-2937\n",
      "Configuration saved in ./tests-10\\checkpoint-2937\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2937\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2937\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2937\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2937\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2937\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-3204\n",
      "Configuration saved in ./tests-10\\checkpoint-3204\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3204\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3204\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3204\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3204\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3204\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-3471\n",
      "Configuration saved in ./tests-10\\checkpoint-3471\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3471\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3471\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3471\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3471\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3471\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-3738\n",
      "Configuration saved in ./tests-10\\checkpoint-3738\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3738\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3738\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3738\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3738\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3738\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-4005\n",
      "Configuration saved in ./tests-10\\checkpoint-4005\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4005\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4005\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4005\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4005\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4005\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-4272\n",
      "Configuration saved in ./tests-10\\checkpoint-4272\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4272\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4272\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4272\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4272\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4272\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-4539\n",
      "Configuration saved in ./tests-10\\checkpoint-4539\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4539\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4539\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4539\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4539\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4539\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-4806\n",
      "Configuration saved in ./tests-10\\checkpoint-4806\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4806\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4806\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4806\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4806\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4806\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-5073\n",
      "Configuration saved in ./tests-10\\checkpoint-5073\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5073\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-5073\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5073\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-5073\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5073\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tokens, sentence, tree.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-5340\n",
      "Configuration saved in ./tests-10\\checkpoint-5340\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-5340\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-5340\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-10\\checkpoint-5340 (score: 0.47020936012268066).\n",
      "Loading module configuration from ./tests-10\\checkpoint-5340\\sst-2-10\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-10'.\n",
      "Loading module weights from ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-10\\checkpoint-5340\\sst-2-10\\head_config.json\n",
      "Overwriting existing head 'sst-2-10'\n",
      "Adding head 'sst-2-10' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-10\\checkpoint-5340 (score: 0.47020936012268066).\n",
      "Saving model checkpoint to ./tests-10\n",
      "Configuration saved in ./tests-10\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\sst-2-10\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor=16, leave_out=[0,1,2,3,4,5,6,7,8,9,10])\n",
    "model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-10\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-10\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-10\"])\n",
    "model2.set_active_adapters(\"sst-2-10\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-10\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b236dd2-8995-4d65-96ec-0d7209410178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Overwriting existing head 'sst-2-10'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.5574660633484163\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_LOCAL_PATH, local_files_only=True)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-10',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-10/sst-2-10/adapter_config.json\")\n",
    "#adapter_name = model2.load_adapter(\"./tests-10/sst-2-10\", config=config)\n",
    "adapter_name = model2.load_adapter(\"./tests-10/sst-2-10\", config=config, model_name=BERT_LOCAL_PATH)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "360866d2-2d8a-4f6e-b793-a472dca0cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-10): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-10): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-10): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-10): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-10): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad00acd4a2d445dc9e1b73789100cacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4962692526594a0fbb0a31344a3a0e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe2a857a2ba4bc6be86bbf47d1986a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1109, 2977, 1110, 17348, 1106, 1129, 1103, 6880, 5944, 112, 188, 1207, 169, 169, 17727, 112, 112, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 7296, 20452, 24156, 11819, 7582, 9146, 117, 2893, 118, 140, 15554, 1181, 3605, 8732, 3263, 1137, 6536, 17979, 1233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5340' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5340/5340 34:57, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.688600</td>\n",
       "      <td>0.660441</td>\n",
       "      <td>0.678474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.630797</td>\n",
       "      <td>0.732062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>0.602197</td>\n",
       "      <td>0.736603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.572355</td>\n",
       "      <td>0.745686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.621800</td>\n",
       "      <td>0.541964</td>\n",
       "      <td>0.754768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.604000</td>\n",
       "      <td>0.513800</td>\n",
       "      <td>0.769301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.573200</td>\n",
       "      <td>0.487139</td>\n",
       "      <td>0.769301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.471312</td>\n",
       "      <td>0.782016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.528300</td>\n",
       "      <td>0.459366</td>\n",
       "      <td>0.790191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.451029</td>\n",
       "      <td>0.796549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.497400</td>\n",
       "      <td>0.444954</td>\n",
       "      <td>0.799273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.491700</td>\n",
       "      <td>0.441650</td>\n",
       "      <td>0.800182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.488600</td>\n",
       "      <td>0.440663</td>\n",
       "      <td>0.800182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.479500</td>\n",
       "      <td>0.439529</td>\n",
       "      <td>0.802906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.477100</td>\n",
       "      <td>0.438487</td>\n",
       "      <td>0.802906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.478100</td>\n",
       "      <td>0.439122</td>\n",
       "      <td>0.802906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.476800</td>\n",
       "      <td>0.437911</td>\n",
       "      <td>0.802906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.437707</td>\n",
       "      <td>0.802906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.476100</td>\n",
       "      <td>0.437202</td>\n",
       "      <td>0.801998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>0.436899</td>\n",
       "      <td>0.802906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-267\n",
      "Configuration saved in ./tests-10\\checkpoint-267\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-267\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-267\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-267\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-267\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-267\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-534\n",
      "Configuration saved in ./tests-10\\checkpoint-534\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-534\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-534\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-534\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-534\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-534\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-801\n",
      "Configuration saved in ./tests-10\\checkpoint-801\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-801\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-801\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-801\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-801\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-801\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-1068\n",
      "Configuration saved in ./tests-10\\checkpoint-1068\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1068\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1068\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1068\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1068\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1068\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-1335\n",
      "Configuration saved in ./tests-10\\checkpoint-1335\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1335\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1335\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1335\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1335\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1335\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-1602\n",
      "Configuration saved in ./tests-10\\checkpoint-1602\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1602\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1602\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1602\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1602\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1602\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-1869\n",
      "Configuration saved in ./tests-10\\checkpoint-1869\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1869\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1869\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1869\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-1869\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-1869\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-2136\n",
      "Configuration saved in ./tests-10\\checkpoint-2136\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2136\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2136\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2136\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2136\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2136\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-2403\n",
      "Configuration saved in ./tests-10\\checkpoint-2403\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2403\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2403\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2403\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2403\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2403\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-2670\n",
      "Configuration saved in ./tests-10\\checkpoint-2670\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2670\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2670\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2670\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2670\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2670\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-2937\n",
      "Configuration saved in ./tests-10\\checkpoint-2937\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2937\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2937\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2937\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-2937\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-2937\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-3204\n",
      "Configuration saved in ./tests-10\\checkpoint-3204\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3204\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3204\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3204\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3204\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3204\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-3471\n",
      "Configuration saved in ./tests-10\\checkpoint-3471\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3471\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3471\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3471\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3471\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3471\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-3738\n",
      "Configuration saved in ./tests-10\\checkpoint-3738\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3738\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3738\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3738\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-3738\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-3738\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-4005\n",
      "Configuration saved in ./tests-10\\checkpoint-4005\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4005\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4005\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4005\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4005\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4005\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-4272\n",
      "Configuration saved in ./tests-10\\checkpoint-4272\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4272\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4272\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4272\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4272\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4272\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-4539\n",
      "Configuration saved in ./tests-10\\checkpoint-4539\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4539\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4539\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4539\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4539\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4539\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-4806\n",
      "Configuration saved in ./tests-10\\checkpoint-4806\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4806\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4806\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4806\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-4806\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-4806\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-5073\n",
      "Configuration saved in ./tests-10\\checkpoint-5073\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5073\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-5073\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5073\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-5073\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5073\\sst-2-10\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-10\\checkpoint-5340\n",
      "Configuration saved in ./tests-10\\checkpoint-5340\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-5340\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\checkpoint-5340\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-10\\checkpoint-5340 (score: 0.4368990659713745).\n",
      "Loading module configuration from ./tests-10\\checkpoint-5340\\sst-2-10\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-10'.\n",
      "Loading module weights from ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-10\\checkpoint-5340\\sst-2-10\\head_config.json\n",
      "Overwriting existing head 'sst-2-10'\n",
      "Adding head 'sst-2-10' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-10\\checkpoint-5340\\sst-2-10\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-10\\checkpoint-5340 (score: 0.4368990659713745).\n",
      "Saving model checkpoint to ./tests-10\n",
      "Configuration saved in ./tests-10\\sst-2-10\\adapter_config.json\n",
      "Module weights saved in ./tests-10\\sst-2-10\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-10\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\sst-2-10\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-10\\sst-2-10\\head_config.json\n",
      "Module weights saved in ./tests-10\\sst-2-10\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor=16, leave_out=[0,1,2,3,4,5,6,7,8,9])\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-11\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-11\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-11\"])\n",
    "model2.set_active_adapters(\"sst-2-11\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-11\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e16fd22-b5b4-4a01-9984-e235ea1728f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\added_tokens.json. We won't load it.\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\special_tokens_map.json. We won't load it.\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\vocab.txt\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer_config.json\n",
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-10' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-10/sst-2-10\\adapter_config.json\n",
      "Adding adapter 'sst-2-10'.\n",
      "Loading module weights from ./tests-10/sst-2-10\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-10/sst-2-10\\head_config.json\n",
      "Overwriting existing head 'sst-2-10'\n",
      "Adding head 'sst-2-10' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-10/sst-2-10\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8117647058823529\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_LOCAL_PATH, local_files_only=True)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-11',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-11/sst-2-11/adapter_config.json\")\n",
    "#adapter_name = model2.load_adapter(\"./tests-10/sst-2-10\", config=config)\n",
    "adapter_name = model2.load_adapter(\"./tests-11/sst-2-11\", config=config, model_name=BERT_LOCAL_PATH)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "725e0f60-6dd7-426a-ab38-945888ba58d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-12' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-12'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-12): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-12): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-12): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-12): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-12): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-12): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-12): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38b6bc58d5243d9aebcf5d5b4504e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d6db38ac3b4c6fa0d20cb11a076b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d1703d4d7e44318be6f84a9481e088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1109, 2977, 1110, 17348, 1106, 1129, 1103, 6880, 5944, 112, 188, 1207, 169, 169, 17727, 112, 112, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 7296, 20452, 24156, 11819, 7582, 9146, 117, 2893, 118, 140, 15554, 1181, 3605, 8732, 3263, 1137, 6536, 17979, 1233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10680' max='10680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10680/10680 1:14:45, Epoch 40/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.666353</td>\n",
       "      <td>0.673933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.675900</td>\n",
       "      <td>0.632749</td>\n",
       "      <td>0.743869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.654900</td>\n",
       "      <td>0.591358</td>\n",
       "      <td>0.741144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.532643</td>\n",
       "      <td>0.772025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.475344</td>\n",
       "      <td>0.780200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.513400</td>\n",
       "      <td>0.444248</td>\n",
       "      <td>0.801090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.475600</td>\n",
       "      <td>0.434366</td>\n",
       "      <td>0.808356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.458100</td>\n",
       "      <td>0.436810</td>\n",
       "      <td>0.802906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.456300</td>\n",
       "      <td>0.431309</td>\n",
       "      <td>0.805631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.449300</td>\n",
       "      <td>0.430215</td>\n",
       "      <td>0.804723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.446500</td>\n",
       "      <td>0.424063</td>\n",
       "      <td>0.808356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.422767</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.422643</td>\n",
       "      <td>0.808356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.422416</td>\n",
       "      <td>0.809264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.436500</td>\n",
       "      <td>0.421360</td>\n",
       "      <td>0.808356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.438700</td>\n",
       "      <td>0.421844</td>\n",
       "      <td>0.810173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.433500</td>\n",
       "      <td>0.418741</td>\n",
       "      <td>0.809264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.417920</td>\n",
       "      <td>0.809264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>0.417698</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.423700</td>\n",
       "      <td>0.420960</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.429900</td>\n",
       "      <td>0.418042</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.416400</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.423900</td>\n",
       "      <td>0.420034</td>\n",
       "      <td>0.815622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.418877</td>\n",
       "      <td>0.814714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>0.416145</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.424700</td>\n",
       "      <td>0.416855</td>\n",
       "      <td>0.816530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>0.419276</td>\n",
       "      <td>0.817439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.415841</td>\n",
       "      <td>0.814714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.416371</td>\n",
       "      <td>0.816530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.415582</td>\n",
       "      <td>0.815622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.414380</td>\n",
       "      <td>0.814714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.418300</td>\n",
       "      <td>0.416296</td>\n",
       "      <td>0.816530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.416031</td>\n",
       "      <td>0.817439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.413937</td>\n",
       "      <td>0.816530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.414695</td>\n",
       "      <td>0.817439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>0.414045</td>\n",
       "      <td>0.815622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.416700</td>\n",
       "      <td>0.413492</td>\n",
       "      <td>0.815622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.413700</td>\n",
       "      <td>0.414473</td>\n",
       "      <td>0.817439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.414258</td>\n",
       "      <td>0.817439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.416500</td>\n",
       "      <td>0.414144</td>\n",
       "      <td>0.816530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-267\n",
      "Configuration saved in ./tests-12\\checkpoint-267\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-267\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-267\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-267\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-267\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-267\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-534\n",
      "Configuration saved in ./tests-12\\checkpoint-534\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-534\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-534\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-534\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-534\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-534\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-801\n",
      "Configuration saved in ./tests-12\\checkpoint-801\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-801\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-801\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-801\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-801\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-801\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-1068\n",
      "Configuration saved in ./tests-12\\checkpoint-1068\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1068\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-1068\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1068\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-1068\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1068\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-1335\n",
      "Configuration saved in ./tests-12\\checkpoint-1335\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1335\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-1335\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1335\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-1335\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1335\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-1602\n",
      "Configuration saved in ./tests-12\\checkpoint-1602\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1602\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-1602\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1602\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-1602\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1602\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-1869\n",
      "Configuration saved in ./tests-12\\checkpoint-1869\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1869\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-1869\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1869\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-1869\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-1869\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-2136\n",
      "Configuration saved in ./tests-12\\checkpoint-2136\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2136\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-2136\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2136\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-2136\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2136\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-2403\n",
      "Configuration saved in ./tests-12\\checkpoint-2403\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2403\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-2403\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2403\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-2403\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2403\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-2670\n",
      "Configuration saved in ./tests-12\\checkpoint-2670\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2670\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-2670\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2670\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-2670\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2670\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-2937\n",
      "Configuration saved in ./tests-12\\checkpoint-2937\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2937\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-2937\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2937\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-2937\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-2937\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-3204\n",
      "Configuration saved in ./tests-12\\checkpoint-3204\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-3204\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-3204\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-3204\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-3204\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-3204\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-3471\n",
      "Configuration saved in ./tests-12\\checkpoint-3471\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-3471\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-3471\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-3471\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-3471\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-3471\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-3738\n",
      "Configuration saved in ./tests-12\\checkpoint-3738\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-3738\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-3738\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-3738\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-3738\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-3738\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-4005\n",
      "Configuration saved in ./tests-12\\checkpoint-4005\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4005\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-4005\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4005\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-4005\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4005\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-4272\n",
      "Configuration saved in ./tests-12\\checkpoint-4272\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4272\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-4272\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4272\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-4272\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4272\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-4539\n",
      "Configuration saved in ./tests-12\\checkpoint-4539\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4539\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-4539\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4539\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-4539\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4539\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-4806\n",
      "Configuration saved in ./tests-12\\checkpoint-4806\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4806\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-4806\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4806\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-4806\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-4806\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-5073\n",
      "Configuration saved in ./tests-12\\checkpoint-5073\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5073\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-5073\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5073\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-5073\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5073\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-5340\n",
      "Configuration saved in ./tests-12\\checkpoint-5340\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5340\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-5340\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5340\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-5340\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5340\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-5607\n",
      "Configuration saved in ./tests-12\\checkpoint-5607\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5607\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-5607\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5607\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-5607\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5607\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-5874\n",
      "Configuration saved in ./tests-12\\checkpoint-5874\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5874\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-5874\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5874\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-5874\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-5874\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-6141\n",
      "Configuration saved in ./tests-12\\checkpoint-6141\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6141\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-6141\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6141\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-6141\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6141\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-6408\n",
      "Configuration saved in ./tests-12\\checkpoint-6408\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6408\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-6408\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6408\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-6408\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6408\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-6675\n",
      "Configuration saved in ./tests-12\\checkpoint-6675\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6675\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-6675\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6675\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-6675\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6675\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-6942\n",
      "Configuration saved in ./tests-12\\checkpoint-6942\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6942\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-6942\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6942\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-6942\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-6942\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-7209\n",
      "Configuration saved in ./tests-12\\checkpoint-7209\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-7209\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-7209\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-7209\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-7209\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-7209\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-7476\n",
      "Configuration saved in ./tests-12\\checkpoint-7476\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-7476\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-7476\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-7476\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-7476\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-7476\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-7743\n",
      "Configuration saved in ./tests-12\\checkpoint-7743\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-7743\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-7743\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-7743\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-7743\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-7743\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-8010\n",
      "Configuration saved in ./tests-12\\checkpoint-8010\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8010\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-8010\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8010\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-8010\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8010\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-8277\n",
      "Configuration saved in ./tests-12\\checkpoint-8277\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8277\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-8277\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8277\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-8277\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8277\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-8544\n",
      "Configuration saved in ./tests-12\\checkpoint-8544\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8544\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-8544\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8544\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-8544\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8544\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-8811\n",
      "Configuration saved in ./tests-12\\checkpoint-8811\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8811\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-8811\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8811\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-8811\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-8811\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-9078\n",
      "Configuration saved in ./tests-12\\checkpoint-9078\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9078\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-9078\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9078\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-9078\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9078\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-9345\n",
      "Configuration saved in ./tests-12\\checkpoint-9345\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9345\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-9345\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9345\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-9345\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9345\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-9612\n",
      "Configuration saved in ./tests-12\\checkpoint-9612\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9612\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-9612\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9612\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-9612\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9612\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-9879\n",
      "Configuration saved in ./tests-12\\checkpoint-9879\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9879\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-9879\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9879\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-9879\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-9879\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-10146\n",
      "Configuration saved in ./tests-12\\checkpoint-10146\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-10146\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-10146\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-10146\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-10146\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-10146\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-10413\n",
      "Configuration saved in ./tests-12\\checkpoint-10413\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-10413\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-10413\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-10413\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-10413\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-10413\\sst-2-12\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-12\\checkpoint-10680\n",
      "Configuration saved in ./tests-12\\checkpoint-10680\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-10680\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-10680\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-10680\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\checkpoint-10680\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\checkpoint-10680\\sst-2-12\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-12\\checkpoint-9879 (score: 0.4134921133518219).\n",
      "Loading module configuration from ./tests-12\\checkpoint-9879\\sst-2-12\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-12'.\n",
      "Loading module weights from ./tests-12\\checkpoint-9879\\sst-2-12\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-12\\checkpoint-9879\\sst-2-12\\head_config.json\n",
      "Overwriting existing head 'sst-2-12'\n",
      "Adding head 'sst-2-12' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-12\\checkpoint-9879\\sst-2-12\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-12\\checkpoint-9879 (score: 0.4134921133518219).\n",
      "Saving model checkpoint to ./tests-12\n",
      "Configuration saved in ./tests-12\\sst-2-12\\adapter_config.json\n",
      "Module weights saved in ./tests-12\\sst-2-12\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-12\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\sst-2-12\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-12\\sst-2-12\\head_config.json\n",
      "Module weights saved in ./tests-12\\sst-2-12\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor=16, leave_out=[0,1,2,3,4,5,6,7,8])\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-12\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-12\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-12\"])\n",
    "model2.set_active_adapters(\"sst-2-12\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-12\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs= 40, #20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0d9775c-ad0b-4133-90f1-64fa07212b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\added_tokens.json. We won't load it.\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\special_tokens_map.json. We won't load it.\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\vocab.txt\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer_config.json\n",
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-12' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-12/sst-2-12\\adapter_config.json\n",
      "Adding adapter 'sst-2-12'.\n",
      "Loading module weights from ./tests-12/sst-2-12\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-12/sst-2-12\\head_config.json\n",
      "Overwriting existing head 'sst-2-12'\n",
      "Adding head 'sst-2-12' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-12/sst-2-12\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.834841628959276\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_LOCAL_PATH, local_files_only=True)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-12',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-12/sst-2-12/adapter_config.json\")\n",
    "#adapter_name = model2.load_adapter(\"./tests-10/sst-2-10\", config=config)\n",
    "adapter_name = model2.load_adapter(\"./tests-12/sst-2-12\", config=config, model_name=BERT_LOCAL_PATH)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b3040c7-318a-4968-a590-1d9f20c7c404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-13' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-13'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-13): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-13): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-13): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=96, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=96, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-13): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=96, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=96, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-13): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2860a635ae24fdaa91988d8a9d74d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b32d29ff5b410191a1587bf91edb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8589ed596fd241cba40ad8ed300e22e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1109, 2977, 1110, 17348, 1106, 1129, 1103, 6880, 5944, 112, 188, 1207, 169, 169, 17727, 112, 112, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 7296, 20452, 24156, 11819, 7582, 9146, 117, 2893, 118, 140, 15554, 1181, 3605, 8732, 3263, 1137, 6536, 17979, 1233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9612' max='10680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9612/10680 1:02:34 < 06:57, 2.56 it/s, Epoch 36/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.688000</td>\n",
       "      <td>0.656714</td>\n",
       "      <td>0.705722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.672400</td>\n",
       "      <td>0.624846</td>\n",
       "      <td>0.744777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>0.589789</td>\n",
       "      <td>0.743869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.550608</td>\n",
       "      <td>0.762035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.606100</td>\n",
       "      <td>0.511957</td>\n",
       "      <td>0.764759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.477255</td>\n",
       "      <td>0.773842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.532400</td>\n",
       "      <td>0.453309</td>\n",
       "      <td>0.792916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.446027</td>\n",
       "      <td>0.799273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.489500</td>\n",
       "      <td>0.440136</td>\n",
       "      <td>0.805631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.437426</td>\n",
       "      <td>0.807448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.471300</td>\n",
       "      <td>0.431561</td>\n",
       "      <td>0.808356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.468200</td>\n",
       "      <td>0.430397</td>\n",
       "      <td>0.809264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.468800</td>\n",
       "      <td>0.429544</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.460900</td>\n",
       "      <td>0.430052</td>\n",
       "      <td>0.808356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>0.428690</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.428740</td>\n",
       "      <td>0.807448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.425943</td>\n",
       "      <td>0.809264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.424703</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.454700</td>\n",
       "      <td>0.423720</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.427543</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.448700</td>\n",
       "      <td>0.423582</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.448600</td>\n",
       "      <td>0.422685</td>\n",
       "      <td>0.814714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.445400</td>\n",
       "      <td>0.425213</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.447400</td>\n",
       "      <td>0.424464</td>\n",
       "      <td>0.813806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.443400</td>\n",
       "      <td>0.422004</td>\n",
       "      <td>0.818347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.444100</td>\n",
       "      <td>0.422728</td>\n",
       "      <td>0.816530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.424944</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.439400</td>\n",
       "      <td>0.421542</td>\n",
       "      <td>0.818347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.438400</td>\n",
       "      <td>0.423568</td>\n",
       "      <td>0.814714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.440600</td>\n",
       "      <td>0.421842</td>\n",
       "      <td>0.818347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.420368</td>\n",
       "      <td>0.818347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.422796</td>\n",
       "      <td>0.815622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.422248</td>\n",
       "      <td>0.816530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>0.420765</td>\n",
       "      <td>0.818347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.420863</td>\n",
       "      <td>0.818347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.440700</td>\n",
       "      <td>0.420575</td>\n",
       "      <td>0.819255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-267\n",
      "Configuration saved in ./tests-13\\checkpoint-267\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-267\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-267\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-267\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-267\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-267\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-534\n",
      "Configuration saved in ./tests-13\\checkpoint-534\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-534\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-534\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-534\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-534\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-534\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-801\n",
      "Configuration saved in ./tests-13\\checkpoint-801\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-801\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-801\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-801\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-801\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-801\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-1068\n",
      "Configuration saved in ./tests-13\\checkpoint-1068\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1068\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-1068\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1068\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-1068\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1068\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-1335\n",
      "Configuration saved in ./tests-13\\checkpoint-1335\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1335\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-1335\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1335\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-1335\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1335\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-1602\n",
      "Configuration saved in ./tests-13\\checkpoint-1602\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1602\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-1602\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1602\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-1602\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1602\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-1869\n",
      "Configuration saved in ./tests-13\\checkpoint-1869\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1869\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-1869\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1869\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-1869\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-1869\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-2136\n",
      "Configuration saved in ./tests-13\\checkpoint-2136\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2136\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-2136\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2136\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-2136\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2136\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-2403\n",
      "Configuration saved in ./tests-13\\checkpoint-2403\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2403\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-2403\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2403\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-2403\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2403\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-2670\n",
      "Configuration saved in ./tests-13\\checkpoint-2670\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2670\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-2670\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2670\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-2670\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2670\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-2937\n",
      "Configuration saved in ./tests-13\\checkpoint-2937\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2937\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-2937\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2937\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-2937\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-2937\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-3204\n",
      "Configuration saved in ./tests-13\\checkpoint-3204\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-3204\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-3204\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-3204\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-3204\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-3204\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-3471\n",
      "Configuration saved in ./tests-13\\checkpoint-3471\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-3471\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-3471\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-3471\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-3471\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-3471\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-3738\n",
      "Configuration saved in ./tests-13\\checkpoint-3738\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-3738\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-3738\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-3738\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-3738\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-3738\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-4005\n",
      "Configuration saved in ./tests-13\\checkpoint-4005\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4005\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-4005\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4005\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-4005\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4005\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-4272\n",
      "Configuration saved in ./tests-13\\checkpoint-4272\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4272\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-4272\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4272\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-4272\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4272\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-4539\n",
      "Configuration saved in ./tests-13\\checkpoint-4539\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4539\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-4539\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4539\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-4539\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4539\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-4806\n",
      "Configuration saved in ./tests-13\\checkpoint-4806\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4806\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-4806\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4806\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-4806\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-4806\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-5073\n",
      "Configuration saved in ./tests-13\\checkpoint-5073\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5073\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-5073\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5073\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-5073\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5073\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-5340\n",
      "Configuration saved in ./tests-13\\checkpoint-5340\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5340\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-5340\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5340\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-5340\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5340\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-5607\n",
      "Configuration saved in ./tests-13\\checkpoint-5607\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5607\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-5607\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5607\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-5607\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5607\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-5874\n",
      "Configuration saved in ./tests-13\\checkpoint-5874\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5874\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-5874\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5874\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-5874\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-5874\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-6141\n",
      "Configuration saved in ./tests-13\\checkpoint-6141\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6141\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-6141\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6141\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-6141\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6141\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-6408\n",
      "Configuration saved in ./tests-13\\checkpoint-6408\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6408\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-6408\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6408\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-6408\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6408\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-6675\n",
      "Configuration saved in ./tests-13\\checkpoint-6675\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6675\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-6675\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6675\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-6675\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6675\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-6942\n",
      "Configuration saved in ./tests-13\\checkpoint-6942\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6942\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-6942\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6942\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-6942\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-6942\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-7209\n",
      "Configuration saved in ./tests-13\\checkpoint-7209\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-7209\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-7209\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-7209\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-7209\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-7209\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-7476\n",
      "Configuration saved in ./tests-13\\checkpoint-7476\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-7476\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-7476\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-7476\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-7476\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-7476\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-7743\n",
      "Configuration saved in ./tests-13\\checkpoint-7743\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-7743\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-7743\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-7743\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-7743\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-7743\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-8010\n",
      "Configuration saved in ./tests-13\\checkpoint-8010\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8010\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-8010\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8010\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-8010\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8010\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-8277\n",
      "Configuration saved in ./tests-13\\checkpoint-8277\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8277\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-8277\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8277\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-8277\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8277\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-8544\n",
      "Configuration saved in ./tests-13\\checkpoint-8544\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8544\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-8544\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8544\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-8544\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8544\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-8811\n",
      "Configuration saved in ./tests-13\\checkpoint-8811\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8811\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-8811\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8811\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-8811\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-8811\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-9078\n",
      "Configuration saved in ./tests-13\\checkpoint-9078\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-9078\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-9078\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-9078\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-9078\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-9078\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-9345\n",
      "Configuration saved in ./tests-13\\checkpoint-9345\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-9345\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-9345\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-9345\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-9345\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-9345\\sst-2-13\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-13\\checkpoint-9612\n",
      "Configuration saved in ./tests-13\\checkpoint-9612\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-9612\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-9612\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-9612\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\checkpoint-9612\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\checkpoint-9612\\sst-2-13\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-13\\checkpoint-8277 (score: 0.42036768794059753).\n",
      "Loading module configuration from ./tests-13\\checkpoint-8277\\sst-2-13\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-13'.\n",
      "Loading module weights from ./tests-13\\checkpoint-8277\\sst-2-13\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-13\\checkpoint-8277\\sst-2-13\\head_config.json\n",
      "Overwriting existing head 'sst-2-13'\n",
      "Adding head 'sst-2-13' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-13\\checkpoint-8277\\sst-2-13\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-13\\checkpoint-8277 (score: 0.42036768794059753).\n",
      "Saving model checkpoint to ./tests-13\n",
      "Configuration saved in ./tests-13\\sst-2-13\\adapter_config.json\n",
      "Module weights saved in ./tests-13\\sst-2-13\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-13\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\sst-2-13\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-13\\sst-2-13\\head_config.json\n",
      "Module weights saved in ./tests-13\\sst-2-13\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'11':8,'10':16}, leave_out=[0,1,2,3,4,5,6,7,8,9])\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-13\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-13\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-13\"])\n",
    "model2.set_active_adapters(\"sst-2-13\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-13\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=40, #20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 5)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83782bc5-3792-4f7b-9caf-69283f8495f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\added_tokens.json. We won't load it.\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\special_tokens_map.json. We won't load it.\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\vocab.txt\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer_config.json\n",
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-13' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-13/sst-2-13\\adapter_config.json\n",
      "Adding adapter 'sst-2-13'.\n",
      "Loading module weights from ./tests-13/sst-2-13\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-13/sst-2-13\\head_config.json\n",
      "Overwriting existing head 'sst-2-13'\n",
      "Adding head 'sst-2-13' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-13/sst-2-13\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8321266968325792\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_LOCAL_PATH, local_files_only=True)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-13',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-13/sst-2-13/adapter_config.json\")\n",
    "#adapter_name = model2.load_adapter(\"./tests-10/sst-2-10\", config=config)\n",
    "adapter_name = model2.load_adapter(\"./tests-13/sst-2-13\", config=config, model_name=BERT_LOCAL_PATH)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c17795-c98d-4a5a-b14e-f473e06e58df",
   "metadata": {},
   "source": [
    "# Baseline adapter with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "942b4fee-82ce-4885-b152-c5736becd2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-14' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-14'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-14): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-14): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-14): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df682dac79d4f3c858070f356e2198f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1486ee2b1584b4b95da779f4e555b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346fcea27f0346d4b15911888f4c7900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1109, 2977, 1110, 17348, 1106, 1129, 1103, 6880, 5944, 112, 188, 1207, 169, 169, 17727, 112, 112, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 7296, 20452, 24156, 11819, 7582, 9146, 117, 2893, 118, 140, 15554, 1181, 3605, 8732, 3263, 1137, 6536, 17979, 1233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5340' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5340/5340 1:00:18, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.651083</td>\n",
       "      <td>0.715713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.589800</td>\n",
       "      <td>0.445808</td>\n",
       "      <td>0.808356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>0.411739</td>\n",
       "      <td>0.814714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.401076</td>\n",
       "      <td>0.822888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>0.407494</td>\n",
       "      <td>0.814714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.400900</td>\n",
       "      <td>0.396454</td>\n",
       "      <td>0.823797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>0.393223</td>\n",
       "      <td>0.826521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.393133</td>\n",
       "      <td>0.827430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.383400</td>\n",
       "      <td>0.391214</td>\n",
       "      <td>0.830154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.391130</td>\n",
       "      <td>0.827430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.380200</td>\n",
       "      <td>0.384743</td>\n",
       "      <td>0.831063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.379500</td>\n",
       "      <td>0.383524</td>\n",
       "      <td>0.831063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.375800</td>\n",
       "      <td>0.384233</td>\n",
       "      <td>0.831063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.372300</td>\n",
       "      <td>0.383508</td>\n",
       "      <td>0.830154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.373000</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.831971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.371100</td>\n",
       "      <td>0.385532</td>\n",
       "      <td>0.833787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.370600</td>\n",
       "      <td>0.382687</td>\n",
       "      <td>0.832879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.369800</td>\n",
       "      <td>0.383290</td>\n",
       "      <td>0.833787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.370800</td>\n",
       "      <td>0.382781</td>\n",
       "      <td>0.834696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.365500</td>\n",
       "      <td>0.382616</td>\n",
       "      <td>0.833787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-267\n",
      "Configuration saved in ./tests-14\\checkpoint-267\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-267\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-267\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-267\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-267\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-267\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-534\n",
      "Configuration saved in ./tests-14\\checkpoint-534\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-534\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-534\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-534\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-534\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-534\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-801\n",
      "Configuration saved in ./tests-14\\checkpoint-801\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-801\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-801\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-801\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-801\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-801\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-1068\n",
      "Configuration saved in ./tests-14\\checkpoint-1068\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1068\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-1068\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1068\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-1068\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1068\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-1335\n",
      "Configuration saved in ./tests-14\\checkpoint-1335\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1335\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-1335\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1335\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-1335\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1335\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-1602\n",
      "Configuration saved in ./tests-14\\checkpoint-1602\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1602\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-1602\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1602\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-1602\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1602\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-1869\n",
      "Configuration saved in ./tests-14\\checkpoint-1869\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1869\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-1869\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1869\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-1869\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-1869\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-2136\n",
      "Configuration saved in ./tests-14\\checkpoint-2136\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2136\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-2136\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2136\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-2136\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2136\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-2403\n",
      "Configuration saved in ./tests-14\\checkpoint-2403\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2403\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-2403\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2403\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-2403\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2403\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-2670\n",
      "Configuration saved in ./tests-14\\checkpoint-2670\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2670\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-2670\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2670\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-2670\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2670\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-2937\n",
      "Configuration saved in ./tests-14\\checkpoint-2937\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2937\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-2937\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2937\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-2937\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-2937\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-3204\n",
      "Configuration saved in ./tests-14\\checkpoint-3204\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-3204\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-3204\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-3204\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-3204\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-3204\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-3471\n",
      "Configuration saved in ./tests-14\\checkpoint-3471\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-3471\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-3471\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-3471\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-3471\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-3471\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-3738\n",
      "Configuration saved in ./tests-14\\checkpoint-3738\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-3738\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-3738\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-3738\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-3738\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-3738\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-4005\n",
      "Configuration saved in ./tests-14\\checkpoint-4005\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4005\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-4005\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4005\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-4005\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4005\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-4272\n",
      "Configuration saved in ./tests-14\\checkpoint-4272\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4272\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-4272\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4272\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-4272\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4272\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-4539\n",
      "Configuration saved in ./tests-14\\checkpoint-4539\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4539\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-4539\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4539\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-4539\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4539\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-4806\n",
      "Configuration saved in ./tests-14\\checkpoint-4806\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4806\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-4806\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4806\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-4806\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-4806\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-5073\n",
      "Configuration saved in ./tests-14\\checkpoint-5073\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-5073\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-5073\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-5073\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-5073\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-5073\\sst-2-14\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, tokens, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-14\\checkpoint-5340\n",
      "Configuration saved in ./tests-14\\checkpoint-5340\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-5340\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-5340\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-5340\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\checkpoint-5340\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\checkpoint-5340\\sst-2-14\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-14\\checkpoint-5340 (score: 0.3826155662536621).\n",
      "Loading module configuration from ./tests-14\\checkpoint-5340\\sst-2-14\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-14'.\n",
      "Loading module weights from ./tests-14\\checkpoint-5340\\sst-2-14\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-14\\checkpoint-5340\\sst-2-14\\head_config.json\n",
      "Overwriting existing head 'sst-2-14'\n",
      "Adding head 'sst-2-14' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-14\\checkpoint-5340\\sst-2-14\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-14\\checkpoint-5340 (score: 0.3826155662536621).\n",
      "Saving model checkpoint to ./tests-14\n",
      "Configuration saved in ./tests-14\\sst-2-14\\adapter_config.json\n",
      "Module weights saved in ./tests-14\\sst-2-14\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-14\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\sst-2-14\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-14\\sst-2-14\\head_config.json\n",
      "Module weights saved in ./tests-14\\sst-2-14\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", non_linearity='relu')\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-14\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-14\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-14\"])\n",
    "model2.set_active_adapters(\"sst-2-14\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-14\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b61becb5-a2bf-4feb-b281-4a892023b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\added_tokens.json. We won't load it.\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\special_tokens_map.json. We won't load it.\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\vocab.txt\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer_config.json\n",
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-14' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-14/sst-2-14\\adapter_config.json\n",
      "Adding adapter 'sst-2-14'.\n",
      "Loading module weights from ./tests-14/sst-2-14\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-14/sst-2-14\\head_config.json\n",
      "Overwriting existing head 'sst-2-14'\n",
      "Adding head 'sst-2-14' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-14/sst-2-14\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8447963800904977\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_LOCAL_PATH, local_files_only=True)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-14',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-14/sst-2-14/adapter_config.json\")\n",
    "#adapter_name = model2.load_adapter(\"./tests-10/sst-2-10\", config=config)\n",
    "adapter_name = model2.load_adapter(\"./tests-14/sst-2-14\", config=config, model_name=BERT_LOCAL_PATH)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aac0afa6-ca94-4eec-83f5-ced0f62f518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-15): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=32, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=32, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-15): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=32, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=32, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-15): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-15): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-15): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-15): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=64, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=64, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-15): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc5142ef94e47baa0f0bb8baadde338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d21d8f5095844528a21c571bd7b6375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ef87a07d854350a600f21737b893d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1109, 2977, 1110, 17348, 1106, 1129, 1103, 6880, 5944, 112, 188, 1207, 169, 169, 17727, 112, 112, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 7296, 20452, 24156, 11819, 7582, 9146, 117, 2893, 118, 140, 15554, 1181, 3605, 8732, 3263, 1137, 6536, 17979, 1233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6675' max='10680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6675/10680 47:15 < 28:21, 2.35 it/s, Epoch 25/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.684600</td>\n",
       "      <td>0.650928</td>\n",
       "      <td>0.710263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.668900</td>\n",
       "      <td>0.616813</td>\n",
       "      <td>0.742961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.645900</td>\n",
       "      <td>0.573311</td>\n",
       "      <td>0.748411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.517451</td>\n",
       "      <td>0.762035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>0.471594</td>\n",
       "      <td>0.774750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.444654</td>\n",
       "      <td>0.796549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.478500</td>\n",
       "      <td>0.436021</td>\n",
       "      <td>0.805631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.437372</td>\n",
       "      <td>0.801998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.460700</td>\n",
       "      <td>0.433105</td>\n",
       "      <td>0.807448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.453300</td>\n",
       "      <td>0.431662</td>\n",
       "      <td>0.805631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>0.425419</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.448900</td>\n",
       "      <td>0.424062</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.448300</td>\n",
       "      <td>0.423217</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.441400</td>\n",
       "      <td>0.424123</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.422839</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.440900</td>\n",
       "      <td>0.423184</td>\n",
       "      <td>0.809264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.438300</td>\n",
       "      <td>0.420027</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.437200</td>\n",
       "      <td>0.419063</td>\n",
       "      <td>0.811081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.418737</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.427300</td>\n",
       "      <td>0.421967</td>\n",
       "      <td>0.809264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.432400</td>\n",
       "      <td>0.418764</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.430300</td>\n",
       "      <td>0.417070</td>\n",
       "      <td>0.811989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.429000</td>\n",
       "      <td>0.421407</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.428300</td>\n",
       "      <td>0.419989</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>0.417181</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-267\n",
      "Configuration saved in ./tests-15\\checkpoint-267\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-267\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-267\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-267\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-267\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-267\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-534\n",
      "Configuration saved in ./tests-15\\checkpoint-534\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-534\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-534\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-534\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-534\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-534\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-801\n",
      "Configuration saved in ./tests-15\\checkpoint-801\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-801\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-801\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-801\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-801\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-801\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-1068\n",
      "Configuration saved in ./tests-15\\checkpoint-1068\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1068\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-1068\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1068\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-1068\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1068\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-1335\n",
      "Configuration saved in ./tests-15\\checkpoint-1335\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1335\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-1335\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1335\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-1335\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1335\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-1602\n",
      "Configuration saved in ./tests-15\\checkpoint-1602\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1602\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-1602\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1602\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-1602\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1602\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-1869\n",
      "Configuration saved in ./tests-15\\checkpoint-1869\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1869\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-1869\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1869\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-1869\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-1869\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-2136\n",
      "Configuration saved in ./tests-15\\checkpoint-2136\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2136\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-2136\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2136\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-2136\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2136\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-2403\n",
      "Configuration saved in ./tests-15\\checkpoint-2403\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2403\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-2403\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2403\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-2403\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2403\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-2670\n",
      "Configuration saved in ./tests-15\\checkpoint-2670\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2670\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-2670\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2670\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-2670\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2670\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-2937\n",
      "Configuration saved in ./tests-15\\checkpoint-2937\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2937\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-2937\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2937\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-2937\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-2937\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-3204\n",
      "Configuration saved in ./tests-15\\checkpoint-3204\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-3204\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-3204\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-3204\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-3204\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-3204\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-3471\n",
      "Configuration saved in ./tests-15\\checkpoint-3471\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-3471\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-3471\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-3471\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-3471\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-3471\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-3738\n",
      "Configuration saved in ./tests-15\\checkpoint-3738\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-3738\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-3738\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-3738\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-3738\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-3738\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-4005\n",
      "Configuration saved in ./tests-15\\checkpoint-4005\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4005\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-4005\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4005\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-4005\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4005\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-4272\n",
      "Configuration saved in ./tests-15\\checkpoint-4272\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4272\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-4272\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4272\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-4272\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4272\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-4539\n",
      "Configuration saved in ./tests-15\\checkpoint-4539\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4539\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-4539\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4539\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-4539\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4539\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-4806\n",
      "Configuration saved in ./tests-15\\checkpoint-4806\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4806\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-4806\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4806\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-4806\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-4806\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-5073\n",
      "Configuration saved in ./tests-15\\checkpoint-5073\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5073\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-5073\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5073\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-5073\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5073\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-5340\n",
      "Configuration saved in ./tests-15\\checkpoint-5340\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5340\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-5340\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5340\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-5340\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5340\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-5607\n",
      "Configuration saved in ./tests-15\\checkpoint-5607\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5607\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-5607\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5607\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-5607\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5607\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-5874\n",
      "Configuration saved in ./tests-15\\checkpoint-5874\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5874\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-5874\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5874\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-5874\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-5874\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-6141\n",
      "Configuration saved in ./tests-15\\checkpoint-6141\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-6141\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-6141\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-6141\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-6141\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-6141\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-6408\n",
      "Configuration saved in ./tests-15\\checkpoint-6408\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-6408\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-6408\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-6408\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-6408\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-6408\\sst-2-15\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-15\\checkpoint-6675\n",
      "Configuration saved in ./tests-15\\checkpoint-6675\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-6675\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-6675\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-6675\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\checkpoint-6675\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\checkpoint-6675\\sst-2-15\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-15\\checkpoint-5874 (score: 0.4170702397823334).\n",
      "Loading module configuration from ./tests-15\\checkpoint-5874\\sst-2-15\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-15'.\n",
      "Loading module weights from ./tests-15\\checkpoint-5874\\sst-2-15\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-15\\checkpoint-5874\\sst-2-15\\head_config.json\n",
      "Overwriting existing head 'sst-2-15'\n",
      "Adding head 'sst-2-15' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-15\\checkpoint-5874\\sst-2-15\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-15\\checkpoint-5874 (score: 0.4170702397823334).\n",
      "Saving model checkpoint to ./tests-15\n",
      "Configuration saved in ./tests-15\\sst-2-15\\adapter_config.json\n",
      "Module weights saved in ./tests-15\\sst-2-15\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-15\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\sst-2-15\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-15\\sst-2-15\\head_config.json\n",
      "Module weights saved in ./tests-15\\sst-2-15\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor={'11':12,'10':16,'9':24}, leave_out=[0,1,2,3,4,5,6,7,8])\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-15\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-15\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-15\"])\n",
    "model2.set_active_adapters(\"sst-2-15\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-15\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=40, #20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95385f11-e309-475e-a837-c28bcf583055",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\added_tokens.json. We won't load it.\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\special_tokens_map.json. We won't load it.\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\vocab.txt\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer_config.json\n",
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-15' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-15/sst-2-15\\adapter_config.json\n",
      "Adding adapter 'sst-2-15'.\n",
      "Loading module weights from ./tests-15/sst-2-15\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-15/sst-2-15\\head_config.json\n",
      "Overwriting existing head 'sst-2-15'\n",
      "Adding head 'sst-2-15' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-15/sst-2-15\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.8316742081447964\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_LOCAL_PATH, local_files_only=True)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-15',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-15/sst-2-15/adapter_config.json\")\n",
    "#adapter_name = model2.load_adapter(\"./tests-10/sst-2-10\", config=config)\n",
    "adapter_name = model2.load_adapter(\"./tests-15/sst-2-15\", config=config, model_name=BERT_LOCAL_PATH)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5af0d-590f-4f5f-9537-267e3cf44665",
   "metadata": {},
   "source": [
    "# Linear Normalization Before and After Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7852a89-153b-4941-a9f6-f69da24e4764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-16' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-16'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-16): Adapter(\n",
      "                  (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                    (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (2): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-16): Adapter(\n",
      "                (adapter_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (1): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (2): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-16): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a1c74614044f11acb846dadddac3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b261b6ba102d492dba79cd97bcf43df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee5a40c7129433daba2fcd04da32fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1109, 2977, 1110, 17348, 1106, 1129, 1103, 6880, 5944, 112, 188, 1207, 169, 169, 17727, 112, 112, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 7296, 20452, 24156, 11819, 7582, 9146, 117, 2893, 118, 140, 15554, 1181, 3605, 8732, 3263, 1137, 6536, 17979, 1233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2937' max='10680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2937/10680 35:57 < 1:34:51, 1.36 it/s, Epoch 11/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.701200</td>\n",
       "      <td>0.687025</td>\n",
       "      <td>0.555858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693300</td>\n",
       "      <td>0.662794</td>\n",
       "      <td>0.614896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.616834</td>\n",
       "      <td>0.682107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.662300</td>\n",
       "      <td>0.606182</td>\n",
       "      <td>0.681199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.644500</td>\n",
       "      <td>0.596259</td>\n",
       "      <td>0.702089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.576442</td>\n",
       "      <td>0.706630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.622700</td>\n",
       "      <td>0.577296</td>\n",
       "      <td>0.707539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.610600</td>\n",
       "      <td>0.571125</td>\n",
       "      <td>0.716621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.576025</td>\n",
       "      <td>0.713896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.579731</td>\n",
       "      <td>0.712988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.590400</td>\n",
       "      <td>0.582375</td>\n",
       "      <td>0.711172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-267\n",
      "Configuration saved in ./tests-16\\checkpoint-267\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-267\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-267\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-267\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-267\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-267\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-534\n",
      "Configuration saved in ./tests-16\\checkpoint-534\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-534\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-534\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-534\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-534\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-534\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-801\n",
      "Configuration saved in ./tests-16\\checkpoint-801\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-801\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-801\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-801\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-801\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-801\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-1068\n",
      "Configuration saved in ./tests-16\\checkpoint-1068\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1068\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-1068\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1068\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-1068\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1068\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-1335\n",
      "Configuration saved in ./tests-16\\checkpoint-1335\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1335\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-1335\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1335\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-1335\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1335\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-1602\n",
      "Configuration saved in ./tests-16\\checkpoint-1602\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1602\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-1602\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1602\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-1602\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1602\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-1869\n",
      "Configuration saved in ./tests-16\\checkpoint-1869\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1869\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-1869\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1869\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-1869\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-1869\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-2136\n",
      "Configuration saved in ./tests-16\\checkpoint-2136\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2136\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-2136\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2136\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-2136\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2136\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-2403\n",
      "Configuration saved in ./tests-16\\checkpoint-2403\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2403\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-2403\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2403\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-2403\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2403\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-2670\n",
      "Configuration saved in ./tests-16\\checkpoint-2670\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2670\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-2670\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2670\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-2670\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2670\\sst-2-16\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-16\\checkpoint-2937\n",
      "Configuration saved in ./tests-16\\checkpoint-2937\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2937\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-2937\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2937\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\checkpoint-2937\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\checkpoint-2937\\sst-2-16\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-16\\checkpoint-2136 (score: 0.5711254477500916).\n",
      "Loading module configuration from ./tests-16\\checkpoint-2136\\sst-2-16\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-16'.\n",
      "Loading module weights from ./tests-16\\checkpoint-2136\\sst-2-16\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-16\\checkpoint-2136\\sst-2-16\\head_config.json\n",
      "Overwriting existing head 'sst-2-16'\n",
      "Adding head 'sst-2-16' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-16\\checkpoint-2136\\sst-2-16\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-16\\checkpoint-2136 (score: 0.5711254477500916).\n",
      "Saving model checkpoint to ./tests-16\n",
      "Configuration saved in ./tests-16\\sst-2-16\\adapter_config.json\n",
      "Module weights saved in ./tests-16\\sst-2-16\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-16\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\sst-2-16\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-16\\sst-2-16\\head_config.json\n",
      "Module weights saved in ./tests-16\\sst-2-16\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor=16, ln_after=True, ln_before=True)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-16\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-16\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-16\"])\n",
    "model2.set_active_adapters(\"sst-2-16\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-16\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=40, #20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d99cf29-c508-4ec3-8fb7-ab54546a7a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\added_tokens.json. We won't load it.\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\special_tokens_map.json. We won't load it.\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\vocab.txt\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer_config.json\n",
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-16' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-16/sst-2-16\\adapter_config.json\n",
      "Adding adapter 'sst-2-16'.\n",
      "Loading module weights from ./tests-16/sst-2-16\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-16/sst-2-16\\head_config.json\n",
      "Overwriting existing head 'sst-2-16'\n",
      "Adding head 'sst-2-16' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-16/sst-2-16\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.7036199095022625\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_LOCAL_PATH, local_files_only=True)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-16',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-16/sst-2-16/adapter_config.json\")\n",
    "#adapter_name = model2.load_adapter(\"./tests-10/sst-2-10\", config=config)\n",
    "adapter_name = model2.load_adapter(\"./tests-16/sst-2-16\", config=config, model_name=BERT_LOCAL_PATH)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba445f6a-a028-4c93-8736-d72c4614b6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-17' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst-2-17'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModelWithHeads(\n",
      "  (bert): BertModel(\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (sst-2-17): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class()\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                    (1): Activation_Function_Class()\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                  (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (sst-2-17): Adapter(\n",
      "                (non_linearity): Activation_Function_Class()\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class()\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "                (adapter_norm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (sst-2-17): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071a39941a754d0d86a8b94cfc081808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ece9ecf3ae4736b50982509ec9bfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb1f9c024424b17b1afa19b2fbf2b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 1109, 2977, 1110, 17348, 1106, 1129, 1103, 6880, 5944, 112, 188, 1207, 169, 169, 17727, 112, 112, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 7296, 20452, 24156, 11819, 7582, 9146, 117, 2893, 118, 140, 15554, 1181, 3605, 8732, 3263, 1137, 6536, 17979, 1233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1.0, 'sentence': \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': \"The|Rock|is|destined|to|be|the|21st|Century|'s|new|``|Conan|''|and|that|he|'s|going|to|make|a|splash|even|greater|than|Arnold|Schwarzenegger|,|Jean-Claud|Van|Damme|or|Steven|Segal|.\", 'tree': '70|70|68|67|63|62|61|60|58|58|57|56|56|64|65|55|54|53|52|51|49|47|47|46|46|45|40|40|41|39|38|38|43|37|37|69|44|39|42|41|42|43|44|45|50|48|48|49|50|51|52|53|54|55|66|57|59|59|60|61|62|63|64|65|66|67|68|69|71|71|0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running training *****\n",
      "  Num examples = 8544\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3204' max='10680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3204/10680 37:48 < 1:28:15, 1.41 it/s, Epoch 12/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.701500</td>\n",
       "      <td>0.697624</td>\n",
       "      <td>0.495005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697300</td>\n",
       "      <td>0.660435</td>\n",
       "      <td>0.609446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.676900</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>0.700272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.639800</td>\n",
       "      <td>0.643782</td>\n",
       "      <td>0.675749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.625100</td>\n",
       "      <td>0.576386</td>\n",
       "      <td>0.711172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.613000</td>\n",
       "      <td>0.567321</td>\n",
       "      <td>0.696639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.601600</td>\n",
       "      <td>0.564905</td>\n",
       "      <td>0.715713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>0.560147</td>\n",
       "      <td>0.722979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.544346</td>\n",
       "      <td>0.729337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.567800</td>\n",
       "      <td>0.555389</td>\n",
       "      <td>0.730245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.562108</td>\n",
       "      <td>0.725704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.561682</td>\n",
       "      <td>0.722979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-267\n",
      "Configuration saved in ./tests-17\\checkpoint-267\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-267\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-267\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-267\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-267\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-267\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-534\n",
      "Configuration saved in ./tests-17\\checkpoint-534\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-534\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-534\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-534\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-534\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-534\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-801\n",
      "Configuration saved in ./tests-17\\checkpoint-801\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-801\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-801\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-801\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-801\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-801\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-1068\n",
      "Configuration saved in ./tests-17\\checkpoint-1068\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1068\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-1068\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1068\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-1068\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1068\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-1335\n",
      "Configuration saved in ./tests-17\\checkpoint-1335\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1335\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-1335\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1335\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-1335\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1335\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-1602\n",
      "Configuration saved in ./tests-17\\checkpoint-1602\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1602\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-1602\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1602\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-1602\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1602\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-1869\n",
      "Configuration saved in ./tests-17\\checkpoint-1869\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1869\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-1869\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1869\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-1869\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-1869\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-2136\n",
      "Configuration saved in ./tests-17\\checkpoint-2136\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2136\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-2136\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2136\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-2136\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2136\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-2403\n",
      "Configuration saved in ./tests-17\\checkpoint-2403\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2403\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-2403\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2403\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-2403\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2403\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-2670\n",
      "Configuration saved in ./tests-17\\checkpoint-2670\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2670\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-2670\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2670\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-2670\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2670\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-2937\n",
      "Configuration saved in ./tests-17\\checkpoint-2937\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2937\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-2937\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2937\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-2937\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-2937\\sst-2-17\\pytorch_model_head.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertModelWithHeads.forward` and have been ignored: tree, sentence, tokens.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1101\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./tests-17\\checkpoint-3204\n",
      "Configuration saved in ./tests-17\\checkpoint-3204\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-3204\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-3204\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-3204\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\checkpoint-3204\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\checkpoint-3204\\sst-2-17\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best adapter(s) from ./tests-17\\checkpoint-2403 (score: 0.544346034526825).\n",
      "Loading module configuration from ./tests-17\\checkpoint-2403\\sst-2-17\\adapter_config.json\n",
      "Overwriting existing adapter 'sst-2-17'.\n",
      "Loading module weights from ./tests-17\\checkpoint-2403\\sst-2-17\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-17\\checkpoint-2403\\sst-2-17\\head_config.json\n",
      "Overwriting existing head 'sst-2-17'\n",
      "Adding head 'sst-2-17' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-17\\checkpoint-2403\\sst-2-17\\pytorch_model_head.bin\n",
      "Loading best adapter fusion(s) from ./tests-17\\checkpoint-2403 (score: 0.544346034526825).\n",
      "Saving model checkpoint to ./tests-17\n",
      "Configuration saved in ./tests-17\\sst-2-17\\adapter_config.json\n",
      "Module weights saved in ./tests-17\\sst-2-17\\pytorch_adapter.bin\n",
      "Configuration saved in ./tests-17\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\sst-2-17\\pytorch_model_head.bin\n",
      "Configuration saved in ./tests-17\\sst-2-17\\head_config.json\n",
      "Module weights saved in ./tests-17\\sst-2-17\\pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://github.com/Adapter-Hub/adapter-transformers/blob/master/examples/text-classification/run_glue_alt.py\n",
    "\n",
    "# Variable adapter sizes\n",
    "# bigger adapter size for higher layers.\n",
    "# reduction_factor (int or Mapping) – Either an integer specifying the reduction factor for all layers \n",
    "# or a mapping specifying the reduction_factor for individual layers. \n",
    "# If not all layers are represented in the mapping a default value should be given e.g. {‘1’: 8, ‘6’: 32, ‘default’: 16}\n",
    "config2 = AdapterConfig.load(\"houlsby\", reduction_factor=16, ln_after=True, ln_before=False)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\", config=config2, num_labels=2)\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "\"\"\"\n",
    "# Add classification head\n",
    "num_labels = 0\n",
    "label_list = []\n",
    "is_regression = dataset[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    # A useful fast method:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
    "    label_list = datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()  # Let's sort it for determinism\n",
    "    num_labels = len(label_list)\n",
    "\"\"\"\n",
    "\n",
    "# Add classification head\n",
    "num_labels = 2\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "# Ref: https://docs.adapterhub.ml/prediction_heads.html\n",
    "model2.add_classification_head(\n",
    "    \"sst-2-17\",\n",
    "    num_labels=num_labels,\n",
    "    id2label={i: v for i, v in enumerate(label_list)} if num_labels > 0 else None,\n",
    ")\n",
    "\n",
    "# add a new adapter\n",
    "model2.add_adapter(\n",
    "    \"sst-2-17\",\n",
    "    config=config2\n",
    ")\n",
    "\n",
    "# Enable adapter training\n",
    "# The most crucial step when training an adapter module is to freeze all weights in the model except for those of the adapter. \n",
    "# calling the train_adapterNN() method which disables training of all weights outside the task adapter. \n",
    "model2.train_adapter([\"sst-2-17\"])\n",
    "model2.set_active_adapters(\"sst-2-17\")\n",
    "print(model2)\n",
    "\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['sentence'], padding=True, truncation=True)\n",
    "    tokenized_batch[\"label\"] = [int(round(num)) for num in batch[\"label\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets['train'][0])\n",
    "\n",
    "# It is needed to format the label to long (label must be long for CELoss). If not it is always float even with typecast at token_function. \n",
    "# Check this out: https://discuss.huggingface.co/t/dataset-set-format/1961\n",
    "format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.long}}\n",
    "tokenized_datasets.set_format(**format, columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])\n",
    "#print(type(tokenized_datasets['train'][0]['label']))\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(tokenized_datasets['train'][0])\n",
    "input_tensor = torch.tensor([\n",
    "    tokenizer.convert_tokens_to_ids(tokenized_datasets['train'][0]['tokens'])\n",
    "])\n",
    "logits = model2(input_tensor)\n",
    "print(logits)  # two heads for binary classification\n",
    "print(logits.view(-1, 2))\n",
    "print()\n",
    "\"\"\"\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tests-17\", \n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=40, #20,\n",
    "    weight_decay = 0.01,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # for early stopping\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end=True\n",
    "    #label_names = [\"Negative\", \"Positive\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#trainer.evaluate()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b222be55-eaf7-4686-80ff-5a6720adffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\added_tokens.json. We won't load it.\n",
      "Didn't find file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\special_tokens_map.json. We won't load it.\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\vocab.txt\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\tokenizer_config.json\n",
      "loading configuration file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\config.json\n",
      "Model config BertConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModelWithHeads were initialized from the model checkpoint at D:/cs7643-dl/Project/cs7643-proj-ablation-study/bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModelWithHeads for predictions without further training.\n",
      "Adding head 'sst-2-17' with config {'head_type': 'classification', 'num_labels': 1, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module configuration from ./tests-17/sst-2-17\\adapter_config.json\n",
      "Adding adapter 'sst-2-17'.\n",
      "Loading module weights from ./tests-17/sst-2-17\\pytorch_adapter.bin\n",
      "Loading module configuration from ./tests-17/sst-2-17\\head_config.json\n",
      "Overwriting existing head 'sst-2-17'\n",
      "Adding head 'sst-2-17' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'Negative': 0, 'Positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Loading module weights from ./tests-17/sst-2-17\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var-sized adapter accuracy on SST-2 test data:  0.7226244343891403\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Evaluate the above pre-trained adapter on SST-2 test data \"\"\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_LOCAL_PATH, local_files_only=True)\n",
    "#model2 = AutoModelWithHeads.from_pretrained(\"bert-base-uncased\")\n",
    "model2 = AutoModelWithHeads.from_pretrained(BERT_LOCAL_PATH, local_files_only=True, num_labels=2)\n",
    "\n",
    "model2.add_classification_head(\n",
    "    'sst-2-17',\n",
    "    num_labels=1\n",
    ")\n",
    "config = AdapterConfig.load(\"./tests-17/sst-2-17/adapter_config.json\")\n",
    "#adapter_name = model2.load_adapter(\"./tests-10/sst-2-10\", config=config)\n",
    "adapter_name = model2.load_adapter(\"./tests-17/sst-2-17\", config=config, model_name=BERT_LOCAL_PATH)\n",
    "model2.set_active_adapters(adapter_name)\n",
    "\n",
    "label_list = [\"Negative\", \"Positive\"]\n",
    "\n",
    "sentiment_analysis2 = pipeline(task=\"sentiment-analysis\", model=model2, tokenizer=tokenizer)\n",
    "\n",
    "# 0=negative; 1=positive\n",
    "output_labels = {0: \"Negative\", 1: \"Positive\"}\n",
    "correct_count = 0\n",
    "accuracy = 0\n",
    "test_data_size = dataset[\"test\"].num_rows\n",
    "for i in range(test_data_size):\n",
    "    \n",
    "    # 0=negative; 1=positive\n",
    "    truth = output_labels[round(dataset[\"test\"][i]['label'], 0)]\n",
    "    result = sentiment_analysis2(dataset[\"test\"][i]['sentence'])[0]\n",
    "    \n",
    "    if result['label'] == truth:\n",
    "        correct_count += 1\n",
    "    \n",
    "    print('Progress: %s / %s' % (i+1, test_data_size), end='\\r')\n",
    "    \n",
    "accuracy = correct_count / test_data_size\n",
    "print(\"var-sized adapter accuracy on SST-2 test data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f5dadb-1611-4e46-98c3-b2ca6ad519ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
